a good policy search algorithm needs to strike a balance between being able to explore candidate policies and being able to zero in on good ones . the standard reinforcement learning framework is modelled by a markov decision process m s a p r where at each time step t the agent takes an action a t a at a state s t s and as a result transitions to a new state s t 1 according to p and receives a reward r t according to r the objective of policy search is to determine a policy s a 0 1 parameterized by in our case that specifies how the agent should act at each state s ie . policy gradient methods leverage the problem structure by estimating j and incorporates it into stochastic gradient descent methods in order to arrive at a potential solution quickly . evolutionary strategies in contrast are able to exhibit better exploration by directly injecting randomness into the space of policies via sampling . we compare these methods to the original ppo and es in cartpole cp and bipedalwalker bw . v is updated along with using an additional lossan entropy termcan also be optionally added to the objective to encourage exploration in a t . combining. instead of using the update 8 as in es ppo we directly set to i with the highest returnwe conjecture that this method would work well despite its apparent greediness because i are likely to be decent solutions as a result of the ppo updates . the objective of cp see for es in this setting we represent bywhere f is a fully connected neural network fc 4 100 relu fc 100 1 sigmoid 1 . we perform hyperparameter search for each method to obtain the best configuration and describe the details below . 