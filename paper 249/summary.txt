deep learning dl techniques allow reinforcement learning rl agents to better correlate actions and delayed rewards by modeling the relationship between states actions and their long term impact on rewards leading to better agent generalization the goal of this project is to build a rl agent for playing the game of pool . dqn and a3c to stabilize training . 2 . whereq s a is the current estimate of the q value s is the current state a is the current action r is the reward received by taking action a at state s s is the transitioned next state is the learning rate and is the discount factor of the rewards q table implements q learning using a look up table to keep the q value for each discrete state action pair we use the epsilon greedy method as our exploration strategy where at each time step there is a probability of selecting a random action and probability 1 of selecting the optimal action from the current estimate of the optimal q function . however in an environment with simpler settings and with potentially unlimited resources q table has the advantage of being the simplest implementation and having the best performance q table has produced a particularly interesting result in that at the end of its learning it repeatedly executed an exact set of moves that will complete an episode in 6 moves for a total reward of 4 . 2 . normalization is essential in stabilizing the training in neural networks . without normalization to the inputs it is found that the values tend to explode as the inputs are forwarded to the end of the network and it became difficult for the output values to be tuned back to its normal range hence the output actions are mostly clipped at 1 or 0 . 