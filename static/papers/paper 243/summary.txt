we implement a policy gradient algorithm advantage actor critic a2c and an evolutionary algorithm es for the cartpole problem on openai gym . a class of search algorithms which addresses these problems of drl is the evolutionary strategies es . these algorithms fall under blackbox optimization which estimate the parameters of the policy neural network to optimize the cumulative rewards with no regards to the given environment . while es algorithms are more stable than drl and can explore the feature space better they suffer from low exploitation of the environment feedback signals and tend to have poor performances in most applications due to their high sample complexity . this paper similarly in the work of finally. we combine es and a2c iteratively in a sequence . we use the weighted combination of parameters proportional to the rewards obtained on the episode for this combination . the number of instances of parameters we spawn noises we generate is set to 50 we used a noise standard deviation of 0 1 and a learning rate of 0 001 for the es algorithm . our vanilla evolutionary a2c algorithm takes 226 episodes to reach a reward of 200 averaged over 100 different training sequences and we observe that it performs better than both the standalone algorithms . in our final combination es spawns a population of parameters and a2c updates each member of the population by performing a series of gradient descent updates . we extended our algorithm to the mountain car environment. literature study of previous work on combining policy gradient algorithms with es fatma implementing a standalone a2c kaushik ram implementing a standalone es devang vanilla combination of a2c and es devang and fatma final combination of a2c and es devang and kaushik ram extending combined algorithm to the mountain car problem devang writing the report kaushik ram and fatma. 