jpeg002-Figure3-1.png,a simplified depiction of the recurrent model used at each time step the network outputs an estimate of the object count and an x y coordinate pair specifying the next glimpse location which is used along with the input image to produce the series of focus windows described elsewhere in this paper
jpeg002-Figure1-1.png,a representation of the input to the recurrent network here three windows of decreasing size and increasing resolution are shown
jpeg002-Figure2-1.png,the original image from which the above windows were obtained
jpeg002-Figure4-1.png,comparison of performance of the feedforward network and attention model controlling for training time
jpeg002-Figure5-1.png,demonstration of the efficacy of the glimpse location decision mechanism lower count images after only a few glimpses while higher count images could not indicating that glimpses were indeed targeted at the relevant areas of the image i e the locations of the objects after sufficient training
jpeg002-Figure6-1.png,repeated version of the previous experiment but with the model s field of view restricted entirely to the focus window the effect observed in the original version was not replicated here indicated that the low resolution peripheral image data was indeed enabling proper functioning of the attention mechanism
jpeg003-Figure1-1.png,chinese checkers rules downloaded from website
jpeg003-Figure2-1.png,board representation
jpeg003-TableI-1.png,time used and number of nodes visited in computing the minimax value
jpeg003-Figure4-1.png,effects of weights
jpeg003-Figure5-1.png,effects of search depth
jpeg003-Figure6-1.png,effects of modified strategy
jpeg006-Figure1-1.png,move vs turnleft 2d cross section for k 3
jpeg006-Figure3-1.png,k means clustering for k 3
jpeg006-Figure2-1.png,coordinate clustering for k 3
jpeg006-Figure5-1.png,softmax regression with logistic and clustering for training data
jpeg006-Figure4-1.png,logistic function regression
jpeg008-Figure1-1.png,this graph shows the accuracy of model on the withheld testing data for a variety of choices for the number of topics size of latent feature vectors k given the irrelevance of many of the articles recommmended by the journal selecting k 100 optimized the recall of relevant articles with respect to the precision of the overall model and was high enough to ensure user provided recommendations were maintained with high confidence
jpeg008-Figure2-1.png,this graph shows the model accuracy on the training data with respect to the number of topics size of feature vectors k when conducting the hyperparameter search note that for the purposes of low computation costs parameters were treated as independent when conducting this search and were sequentially optimized although a grid search would have likely been more optimal
jpeg008-Figure4-1.png,this graph shows the proportion of documents that received predicted rating rij in the intervals depicted on the x axis for k 25 including new predictions of articles not originally predicted by the comparative psychology journal model with the yellow bars given the sparseness of the training matrix and also the high confidence in predicting original user recommendations there are a large number of new articles predicted especially as the confidence decreases
jpeg008-Figure5-1.png,this table shows the rankings of the provided article recommendations where as illustrated irrelevant articles received lower rankings than more relevant articles
jpeg008-Figure3-1.png,this graph shows the proportion of documents that received predicted rating rij in the intervals depicted on the x axis for k 25 where a higher rating corresponds to a prediction that user i is more likely to like document j the blue bars represent training data and the green bars represent withheld user information as the data suggest the provided recommendations received high scores as they should given that a user has expressed interest already and many of the withheld documents around 50 occurred as a top 20 prediction for the article
jpeg008-Figure6-1.png,this table shows the rankings of the withheld articles with np indicating that the withheld article in question was not predicted as a similar article of interest by our ctr algorithm note that all of these withheld articles with the possible exception of 1 that were not recommended were classified as or irrelevant to the user
jpeg008-Figure7-1.png,this table shows the rankings of the new recommendations within the top 20 provided by our recommendation platform that were not present in the list of similar items generated by the ijcp content based recommendation
jpeg009-Figure4-1.png,energy predicted by regression tree versus true energy for a sample of test pulses using 2 template method 13 error 4a predicted energy using 15 templates generated with k means clustering 0 1 error
jpeg009-Figure5-1.png,wfs with noise and the energy prediction for these wfs observed 17 error on 10k wf test set in measurement of energy for collection signals in addition many of the induction signals were assigned large energies
jpeg009-Figure1-1.png,templates of induction and collection signals used in the optimal filter in the time domain signals represent pure collection and induction pulses
jpeg009-Figure3-1.png,templates generated using k means clustering with 10 clusters appears to find both collection like and induction like signals
jpeg009-Figure2-1.png,projection of waveforms into collection and induction space using the 2 templates generated with a charge deposit in the detector center
jpeg010-Figure2-1.png,test accuracy with various learning algorithms
jpeg010-Figure4-1.png,tokens with positive or negative weight from naive bayes predictor in unstemmed form for readability
jpeg010-Figure5-1.png,mdp returns vs random guesser for tesla jan 01 2015 to dec 31 2015
jpeg010-Figure3-1.png,tabulated test accuracies from incrementally optimised algorithms
jpeg010-Figure1-1.png,webscraping pipeline
jpeg011-Figure1-1.png,a schematic of the checkerboard discrimination task design b location of pmd on a macaque brain 101 units were recorded from this region
jpeg011-Figure2-1.png,a firing rates from 10 example units during a left and right reach data is aligned to movement onset blue line b average firing rate across all neurons during the reach t 0 refers to movement onset dotted lines refer to right reaches c pca performed on the neural activity black circles correspond to 300 ms prior to movement onset while the blue circles correspond to movement onset
jpeg011-Figure3-1.png,a accuracy of three classifiers that aim to classify the motor choice from neural activity prior to during and after movement onset b accuracy of three classifiers that aim to classify the overrepresented color from neural activity prior to during and after stimulus presentation bold lines are the mean shaded area represents the standard error across the 10 folds
jpeg011-Figure4-1.png,projection of neural data onto the first two components pc1 and pc2 for different time points across the trial for the first data fold each point corresponds to a single trial the color of the point corresponds to the reach direction note that the color labels switch because the sign is non identifiable
jpeg011-Figure5-1.png,classifier accuracy for kernelized svm and logistic regression with inputs projected onto the first 5 10 or 101 principal components for a short reaction times and b slow reaction times pale colors correspond to decoders that use pca processed inputs
jpeg011-Figure6-1.png,the histograms show the decoding accuracy of logistic regression classifiers that use single neuron activity 101 classifiers in total the plots on the right show the decoding accuracy of a decoding using 1 neuron 2 neurons 3 neurons etc the numbers correspond to how the neurons are labeled in the data labeled in chronological order the dotted line gives the mean decoding accuracy when using full population data
jpeg011-Figure7-1.png,observing how well decoders from specific times perform across other times in the trial grey movement onset decoder red pre movement movement decoder dark purple green optimal decoder from figure 3a top row results for short rt bottom row results for long rt
jpeg012-Figure4-1.png,observed images at different time steps k
jpeg012-Figure5-1.png,classification accuracy averaged over 20 simulations
jpeg012-Figure3-1.png,the initial observation and the fitted trajectories
jpeg012-Figure2-1.png,the trajectory codebook used in the simulation
jpeg012-Figure1-1.png,bayesian network structure of the trajectory classification problem
jpeg013-Figure4-1.png,thresholding the normalized fat signal dataset gives a binary mask corresponding to voxels to be associated with fat based tissues
jpeg013-Figure5-1.png,the water and fat features were combined to provide a binary mask to encourage the separation of tissue voxels from background noise
jpeg013-Figure8-1.png,the median intensity value over each region in was assigned to every voxel within that region to promote uniform classification by region and voxel intensity
jpeg013-Figure6-1.png,each connected region in the volume was assigned a unique integer label the labels were then normalized to a maximum value of one
jpeg013-Figure7-1.png,thresholding out the largest of the regions leaves a fair estimate of voxel regions expected to relate to the connective tissues class
jpeg013-Figure12-1.png,the k means algorithm produces an integer labeled mapping of the dataset a 2d slice of the labeled tissues with each tissue class represented by a different grayscale value is provided in order of increasing whiteness the classes are background black fat muscle skin and connective tissue white
jpeg013-Figure1-1.png,selected slice from the water only dataset gray levels closer to white indicate a stronger water signal at that pixel
jpeg013-Figure3-1.png,thresholding the normalized water signal dataset gives a binary mask corresponding to voxels to be associated with waterbased tissues
jpeg013-Figure2-1.png,selected slice from the fat only dataset gray levels closer to white indicate a stronger fat signal at that pixel
jpeg013-Figure10-1.png,the convolution described in eqn 3 produces a volume with local horizontal gradients represented by voxel intensity
jpeg013-Figure9-1.png,taking a weighted logarithm of the vertical voxel locations enforces the prior expectation regarding the spatial distribution of abdominal tissues
jpeg014-Figure4-1.png,sample spds at bytes and the circle
jpeg014-Figure5-1.png,variance explained vs of principal components
jpeg014-Figure3-1.png,sample mfccs at bytes and the circle
jpeg014-Figure6-1.png,rains vs tressider using the first 3 pcs
jpeg014-Figure7-1.png,oval vs circle using the first 3 pcs
jpeg014-Figure11-1.png,human confusion matrix
jpeg014-Figure10-1.png,confusion matrix with balanced classes
jpeg014-Figure9-1.png,overall confusion matrix
jpeg014-Figure1-1.png,system block diagram
jpeg014-TableII-1.png,classifier comparison
jpeg014-Figure8-1.png,error vs number of subsamples
jpeg016-Figure2-1.png,num words remaining vs threshold for words to be removed
jpeg016-Figure1-1.png,correlation plot of features conditional on 100 000 closing price bucket
jpeg016-Figure4-1.png,residual plot on the testing set
jpeg016-Figure5-1.png,boxplot for residuals on the testing set
jpeg016-Figure3-1.png,normal distribution of residuals on the test set
jpeg016-Figure6-1.png,histogram to show assymetry in residuals for price ranges 300 000 to 400000
jpeg017-Figure5-1.png,mean average precision using different values of k for our kernel methods the algorithms are numbered in order of discussion in this section
jpeg017-Figure4-1.png,pearson correlation matrix for the processed features
jpeg017-Figure3-1.png,mean average precision using different values of k for our baseline methods
jpeg017-Figure2-1.png,the distribution of the target hotel clusters
jpeg017-Figure6-1.png,precision recall f1 and map5 scores for each algorithm across the chosen subset of the data map5 refers to mean average precision over 5 recommendations
jpeg017-Figure1-1.png,threedimensional pca plot of the three most popular hotel clusters each is given a different color
jpeg018-Figure1-1.png,two possible realizations of a deltaic reservoir obtained using direct sampling highlighted portions show how the two realizations are different from each other
jpeg018-Figure3-1.png,a 3 3 filter for cell similar filters are used for remaining grid cells
jpeg018-Figure2-1.png,two possible realizations of a stone wall obtained using direct sampling
jpeg018-Figure5-1.png,left a 1d patch is shown in orange all the highlighted cells have a common filter right simulation result after training filters constant in patches
jpeg018-Figure4-1.png,simulations of the two scenarios using the trained filters
jpeg019-Figure4-1.png,training data top row 10cm cylindrical water phantom with of 2cm calcium inserts testing data 7 1x7 1cm water phantom with 1 4x1 4cm calcium inserts
jpeg019-Figure3-1.png,schematic diagram of neural network
jpeg019-Figure2-1.png,schematic diagram of the process for calculating the input of the neural network model
jpeg019-Figure5-1.png,empirical error on the hold out cross validation set left and average absolute thickness of calcium middle and water right
jpeg019-Figure6-1.png,the results of network with one hidden layer number of elements of hidden layer 13 left and with two hidden layers number of elements in each hidden layer 3 ë 0 01 right
jpeg019-Figure1-1.png,example comparison of spectra incident on detector blue and spectra detected after pulse pileup effect green no object is placed in the x ray beam path left and 9cm of water and 1 cm of calcium are placed right
jpeg021-Figure5-1.png,network packet types in nsl kdd dataset
jpeg021-Figure6-1.png,normalized log likelihood of attack types vs number of timesteps
jpeg021-Figure1-1.png,example of mdp controller controller has actions ai and ah action ai encodes the action space of the ids i e whether to pass or drop a packet action ah encodes the action space of the host i e whether to wait for a packet or reset 5
jpeg021-Figure4-1.png,markov adversary model
jpeg021-Figure3-1.png,naive bayes adversary model
jpeg021-Figure2-1.png,naive bayes network packet model
jpeg021-Figure8-1.png,raw log likelihood of attack types vs number of timesteps
jpeg021-Figure7-1.png,log likelihood for malicious and normal packets for different attack types vs number of timesteps
jpeg021-Figure9-1.png,svm validation accuracy for predicting malicious data
jpeg023-Figure4-1.png,red and blue content of all pixels in a sample image blue points were categorized by the em algorithm as background and red points were categorized as jelly contour lines for the two probability distributions are also shown
jpeg023-Figure5-1.png,image segmentation methods a raw image b jelly segmented using a grayscale threshold c jelly segmented using em based technique
jpeg023-Figure3-1.png,the orientation is bucketed into one of 14 categories 1 12 for the in plane angle from vertical and 13 14 for the 3d out of plane angle
jpeg023-Figure2-1.png,jelly orientation a manual estimate of the jelly s orientation is shown in red the estimated orientation for this example is 44 category 2 with a depth of 0
jpeg023-Figure8-1.png,test images confusion matrix
jpeg023-Figure6-1.png,50 strongest features found by the surf algorithm a jelly image with tentacles b jelly image with tentacles removed
jpeg023-Figure7-1.png,training images confusion matrix
jpeg023-Figure1-1.png,sample paraphyllina image mbari
jpeg024-Figure1-1.png,an example of input data
jpeg024-Table1-1.png,selected input and target features
jpeg024-Figure2-1.png,performance of different classifiers
jpeg024-Figure3-1.png,comparison of performance performance of dts regression of different classifiers for rate prediction
jpeg024-Figure5-1.png,relative importance of different input featuers
jpeg027-Figure8-1.png,the results of k 2 ward hierarchical clustering using euclidean distance on the pca squiggle set we then projected the clusters onto the first two linear discriminant functions
jpeg027-Figure7-1.png,the presence of 4 clusters may imply the existence of 4 distinct sources which may be further investigated by the seti team
jpeg027-Figure4-1.png,significance features from the different models
jpeg027-Figure5-1.png,silhouette scores from different methods
jpeg027-Figure3-1.png,the roc curve of the logistic family smooths out at the top bend resulting in a higher auc metric the roc curve of tree based methods is less smooth
jpeg027-Figure6-1.png,principal components visualizations for different clustering schemes
jpeg027-Figure1-1.png,squiggle spectrograms
jpeg027-Figure2-1.png,non squiggle spectrograms
jpeg028-Figure1-1.png,2152 decline curves used to learn to predict production in the future
jpeg028-Figure2-1.png,high productivity good fit upper left high productivity bad fit upper right
jpeg028-Figure4-1.png,predicted curves with l known months increasing for a bad fitting error vs l
jpeg028-Figure3-1.png,predicted curves with l known months increasing for a good fitting error vs l
jpeg028-Figure5-1.png,high productivity good fit upper left high productivity bad fit upper right
jpeg028-Figure6-1.png,decline curves in the high productivity wells left and the low productivity wells right
jpeg028-Figure8-1.png,errors of three methods calculated from leave one out cross validation
jpeg028-Figure7-1.png,high productivity good fit upper left high productivity bad fit upper right
jpeg029-TableV-1.png,linear discriminant analysis results
jpeg029-TableII-1.png,medicare gov estimator results
jpeg029-TableIII-1.png,hrr baseline estimator results
jpeg029-Figure1-1.png,training test set deviance and most important features for hrr baseline estimator regression
jpeg029-TableIV-1.png,hrr regression results
jpeg029-Figure2-1.png,training test set deviance and most important features for hrr regression
jpeg029-Figure8-1.png,gradient boosting learning curve produced from hospital level data with training sets of varying sizes
jpeg029-Figure7-1.png,svm learning curve produced from hospital level data with training sets of varying size
jpeg029-Figure4-1.png,hospital level data projected onto two dimensions using pca
jpeg029-Figure5-1.png,hospital level data projected onto two dimensions using various manifold learning techniques
jpeg029-TableVI-1.png,hospital regression results
jpeg029-Figure3-1.png,training test set deviance and most important features for hospital level regression
jpeg029-Figure6-1.png,linear regression learning curve produced from hospital level data with training sets of varying size
jpeg029-TableVII-1.png,dahc hospital level k means clusters
jpeg029-TableI-1.png,raw data files
jpeg030-Figure2-1.png,fig 1
jpeg030-Figure5-1.png,fig 5b
jpeg030-Figure9-1.png,fig 9a
jpeg031-Figure4-1.png,dropout probability versus test auroc
jpeg031-Figure3-1.png,example hyperparameter tuning plots for single motif embedding
jpeg031-Figure2-1.png,single motif embedding data needs
jpeg031-Figure8-1.png,deeplift interpretation sequence logos multi task classification problem
jpeg031-Figure1-1.png,tal1 encode transcription factor motif depicted as sequence logo
jpeg031-Figure5-1.png,multi motif embedding encode motif sequence logos
jpeg031-Figure6-1.png,data size needs by task multiple motif embedding
jpeg031-Figure7-1.png,hyperparameter tuning plots combined for all 3 tasks
jpeg032-Figure1-1.png,star rating accuracy generated with various algorithms under various dataset sizes
jpeg032-Table2-1.png,detailed n gram configurations
jpeg032-Figure3-1.png,star rating accuracy and polarity accuracy generated with various feature configurations
jpeg032-Figure2-1.png,polarity accuracy generated with various algorithms under various dataset sizes
jpeg032-Table1-1.png,accuracy of the predictors trained with different class balance techniques
jpeg032-Figure4-1.png,aspect accuracy generated using an overall predictor and an aspect specific predictor
jpeg034-Figure2-1.png,comparing the effects of bandwidth on the neural target and combined models
jpeg034-Figure6-1.png,weighted linear regression based on photo features and average pattern of neural response
jpeg034-Figure1-1.png,structure of the online dating task for the neuroimaging study
jpeg034-Figure3-1.png,eliminating features from the neural model
jpeg035-Table2-1.png,logistic regression error rates for each feature set
jpeg035-Table1-1.png,logistic regression predicting being independent
jpeg036-Figure5-1.png,comparison of 45 deflected transmitted light electric field of the predicted si structure and the ideal 100 efficiency electric field
jpeg036-Figure2-1.png,neural networks with 2 hidden layers
jpeg036-Figure4-1.png,learning curve with two layer neural networks for a nearfield electric field training b far field electric field training
jpeg036-Figure3-1.png,a 1d si structure aligned along x direction at height z 1000 and b the corresponding output electric field given light normal incident on si structures
jpeg036-Figure1-1.png,schematic of 1d si structure with simulation procedure
jpeg037-Figure3-1.png,winning rates against heuristic ai
jpeg037-Figure2-1.png,learning curve for final state yellow and state pair cyan reinforcement learning models
jpeg037-Figure1-1.png,learning curve for supervised linear blue and deep neural net red models
jpeg039-Figure11-1.png,t sne visualization of found clusters
jpeg039-Figure1-1.png,number of applications received by lending club per year
jpeg039-Figure3-1.png,regression task processed lending club data for 2015
jpeg039-Figure2-1.png,classification task processed lending club data for 2015
jpeg039-Figure4-1.png,f measure values by algorithm and year
jpeg039-Figure5-1.png,confusion matrices for year 2009
jpeg039-Figure6-1.png,debt to income of approved applications 2007 2015
jpeg039-Figure8-1.png,most significant features by year
jpeg039-Figure7-1.png,loan amounts of approved applications 2007 2015
jpeg039-Figure9-1.png,degree of variance captured by pca
jpeg039-Figure10-1.png,regression results for interest rate
jpeg040-Figure1-1.png,raw emg signal from a single channel
jpeg040-Figure2-1.png,experimental setup used in 2 to acquire emg data
jpeg040-TableI-1.png,the definitions for the features used for each channel
jpeg040-Figure4-1.png,test accuracy vs number of features per channel for lda and nb classification
jpeg040-Figure3-1.png,windowed emg signal for which we extracted features
jpeg040-TableII-1.png,the feature sets chosen for each classifier
jpeg040-Figure5-1.png,confusion matrix for lda gesture only classification
jpeg040-TableIV-1.png,training and testing accuracy for gesture and force classification 18 classes
jpeg040-Figure6-1.png,confusion matrix for nb gesture only classification
jpeg040-TableIII-1.png,training and testing accuracy for gesture classification 6 classes
jpeg040-Figure7-1.png,confusion matrix for svm gesture only classification classification
jpeg041-Table1-1.png,dataset head
jpeg041-Figure1-1.png,decision tree classifier
jpeg041-Figure2-1.png,hybrid model flow chart
jpeg041-Figure7-1.png,user profile detection package user is the user booked the hotel with the flight
jpeg041-Table2-1.png,accuracy comparison
jpeg041-Figure4-1.png,svd converge
jpeg041-Figure5-1.png,svd rank and rmse
jpeg041-Figure3-1.png,cosine threshold
jpeg041-Figure6-1.png,clustered utility matrix left and utility matrix after svd
jpeg043-Figure1-1.png,distribution of scores of reviews
jpeg043-Figure3-1.png,sentence representation by parsing tree
jpeg043-Figure2-1.png,sentence representation by averaging
jpeg043-Figure4-1.png,3 layers gru
jpeg043-Figure5-1.png,lstm for generation
jpeg043-Figure6-1.png,latent factor model
jpeg043-Figure8-1.png,accuracy table
jpeg043-Figure10-1.png,generated review
jpeg043-Figure7-1.png,3 layers gru loss function
jpeg043-Figure9-1.png,level 5 generation
jpeg043-Figure11-1.png,error decreases as iteration grows different learning rate
jpeg043-Figure14-1.png,cross validation accuracy curve for qda and for dt
jpeg043-Figure12-1.png,recommendation result example
jpeg043-Figure13-1.png,tag cloud for scores from 1 to 5
jpeg044-Figure1-1.png,predicted margins vs fluorescence for four different kernels see the following table for the labels the vertical red line indicates the fluorescence cutoff below which aptamers are labeled as poor and the horizontal red line indicates the zero margin below which aptamers were predicted to be poor that is dots in the upper left quadrant are misclassified as good aptamers while dots in the lower right quadrant are misclassified as poor aptamers
jpeg044-Figure4-1.png,the left graph shows the actual zero mean fluorescences in black and the expected mean function in red against the ranked aptamer following one iteration of the gp ucb algorithm the right graph shows the same following two hundred iterations
jpeg044-Figure3-1.png,the left graph shows the predicted margins vs fluorescence similar to those in figure 1 and the right graph shows the predicted margins vs ranked aptamer for the weighted mismatch kernel this kernel performed the best with a training accuracy of 89 4 and a test accuracy of 85 6
jpeg044-Figure2-1.png,training and test accuracy for the svms in
jpeg045-Figure1-1.png,test accuracies of each model on the babi en 10k task set with 10 000 questions per task legend i lstm with attention ii pyramid lstm iii memn2n iv lstmmemn2n
jpeg045-Table1-1.png,accuracies of scores of each task for the models
jpeg046-Figure2-1.png,the number of circles predicted may not always correlate with the ground truth number of circles
jpeg046-Figure4-1.png,lda performs better for most trials when we predict the number of circles using aicc selection rather than using the number of ground truth circles
jpeg046-Figure3-1.png,the average ber scores across the four algorithms that were tested ber scores are obtained by subtracting the average ber error from 1 so a higher score means better performance
jpeg046-Figure1-1.png,graphical interpretation of an ego network the large red node signifies the ego node the alters are colored according to the ground truth circles they were placed in
jpeg047-Figure1-1.png,a zoomed out view of the road network that we trained and tested our q learning algorithm on the small yellow dots are cars traveling through the network while the larger yellow rectangles represent induction loop sensors
jpeg047-Figure3-1.png,average amount of co2 emissions per distance traveled by cars in the simulation
jpeg047-Figure2-1.png,average number of cars waiting at stoplights during the simulation all differences are highly significant
jpeg049-TableI-1.png,parameter tuning for selected models
jpeg049-Figure2-1.png,10 fold cv showing multinomial deviance of lasso fit
jpeg049-TableIV-1.png,sample data dictionary
jpeg049-Figure3-1.png,performance comparison of different classifiers for analyst recommendations
jpeg049-TableIII-1.png,results for regression stock price prediction
jpeg050-Table1-1.png,sample number in clusters
jpeg050-Figure3-1.png,weight distribution of reading list and recommend reports
jpeg050-Figure2-1.png,weight distribution of reading list
jpeg050-Figure1-1.png,clusters
jpeg050-Table2-1.png,most frequent words in each cluster
jpeg051-Table2-1.png,a matrix of kullback leibler divergences for the pca kde approach
jpeg051-Figure4-1.png,example roc curves for the same samples shown in top and bottom
jpeg051-Figure5-1.png,box plots of auc values for 10 fold cross validation results
jpeg051-Table3-1.png,a matrix of kullback leibler divergences for the auto kde approach
jpeg051-Figure6-1.png,accuracy of svm methods by varying í one and two class í support vector machines a simpler prediction model was created using svm the one class svm was trained on 90 of the experimental sequences while the two class svm used both the
jpeg051-Figure3-1.png,t sne of the experimental data
jpeg051-Figure2-1.png,the 36 features generated using the 9 positions and 4 physiochemical amino acid properties
jpeg051-Table4-1.png,prediction of h1n1 proteome binding
jpeg051-Table1-1.png,a short summary of the sequences used
jpeg051-Figure1-1.png,simple illustrations of hla function in both healthy and infected cells
jpeg052-Figure5-1.png,cboe pcr and s p 500 time series
jpeg052-Figure6-1.png,scatter plots of a raw and b ema smoothed returns at day t vs returns pcr fractional change pcpdev at day t 1
jpeg052-Figure7-1.png,adaboost learning curves for 6 different sets of features over 100 iterations 10 fold cv blue training error black test error
jpeg052-Figure8-1.png,ema returns vs adaboost margin red 3rd order regression line
jpeg052-Figure1-1.png,a histogram b intensity map of sma returns vs put volume
jpeg052-Figure4-1.png,scatter plot of regression coefficients of 80 independent variables intercept for 57 companies
jpeg052-Figure3-1.png,elastic net results for y t y t 1 y t 2 x t 1 x t 2
jpeg052-Figure2-1.png,fig 2 cross correlation of returns and implied volatility spread for a raw b smoothed time series
jpeg052-Table1-1.png,results for boosting with different feature sets
jpeg053-Figure4-1.png,nicely converged 1st layer weights left vs noisy weights right
jpeg053-Figure5-1.png,first layer outputs 2nd row shows main features and local objects are captured 3rd row shows some noise is captured
jpeg053-Figure3-1.png,our pipeline applying svm on extracted features
jpeg053-Figure2-1.png,caffenet architecture
jpeg053-Figure8-1.png,plot of our results
jpeg053-Figure6-1.png,layer 1 outputs same column is from the same model from left to right k 15 k 11 k 7
jpeg053-Figure7-1.png,bow features
jpeg053-Figure1-1.png,caption
jpeg054-Figure2-1.png,comparison of the distributions of mean word length and mean sentence length in the speeches from hillary clinton and donald trump we see that trump generally uses slightly shorter words than clinton on average while trump s sentences are significantly shorter than clinton s on average
jpeg054-Figure1-1.png,comparison of the distributions of frequency with which hillary clinton and bernie sanders speak about immigration and the economy we see that sanders speaks more about the economy and immigration than does clinton
jpeg054-TableI-1.png,feature set
jpeg054-Figure3-1.png,relative mean values for each candidate for each feature
jpeg057-Figure4-1.png,test accuracies of models with highest validation performance on both the bless and lhd sets f1 score for piecewise projection classifier
jpeg057-Figure5-1.png,accuracies for the piecewise projection and stacked lstm models over the lhd set
jpeg057-Table1-1.png,dataset comparison the top 15 most common hyponyms and hypernyms in the bless and weeds data the bless data contains 1337 pairings while the weeds data contains 2564 11
jpeg057-Figure1-1.png,plots of the 3 principal components for each data set left bless right linked hypernyms
jpeg057-Table2-1.png,indicative pairs all indicative pairs for k 30 clusters
jpeg057-Figure3-1.png,f1 score of training phase for k clusters and a threshold ä left 2 7k pairs bless right 1 5m pairs lhd
jpeg057-Figure2-1.png,analogy task accuracy of word vector models the accuracy of the model with optimal parameters is shown as the black dot parameters are fixed at d 200 c 5 sn 3
jpeg058-Table3-1.png,summary of classification results after adjustments for unbalanced data
jpeg058-Figure2-1.png,visualized classification results for training on all sentences green represents correct classifications while red represents incorrect classification names redacted for this paper are indicated with name
jpeg058-Figure1-1.png,text processing overview o i and d represent original inserted and deleted labels
jpeg058-Table1-1.png,summary of classification results for training on all sentences edited token accuracy is defined as number of true positives for inserted tokens number of true positives for deleted tokens total number of inserted tokens total number of deleted tokens boundary errors are not taken into account when calculating precision and recall
jpeg058-Table2-1.png,confusion matrices for training on all sentences
jpeg059-Figure1-1.png,visualization of the three news source models left number articles event center whether reported on event right normalized number of articles event pca of 600 event dimensions to 2d plotted are 100 news sources with most articles about the israel in the summer of 2014
jpeg060-TableI-1.png,rewards table
jpeg060-TableII-1.png,performance comparison
jpeg060-Figure4-1.png,performance comparison among 3 different algorithms
jpeg060-Figure3-1.png,learning curves from sarsa
jpeg060-Figure2-1.png,learning curves from q learning
jpeg060-Figure1-1.png,game snake
jpeg061-Figure1-1.png,a car like vehicle no slip is assumed so differential constraints must be obeyed which limit the vehicle s movement abilities in the lateral direction
jpeg061-Figure2-1.png,the path of a car as it moves through time in an unknown environment it s knowledge of the environment improves as it progresses green is the executed path magenta is the planned path given its current knowledge pink is the boundary into the unknown and blue are the inferred walls
jpeg061-Figure4-1.png,all possible quarter second actions for a car with a max speed of 32 m s
jpeg061-Figure3-1.png,computing ics the blue line depicts a candidate action with green and red showing valid and invalid emergency stopping maneuvers respectively even though the grey un seen space is obstacle free the vehicle doesn t know that from its initial position so to be conservative it assumes such space is an obstacle
jpeg061-Figure5-1.png,maps used to train the machine learning planner
jpeg061-TableI-1.png,time for completion of multiple maps all results in seconds
jpeg061-Figure6-1.png,simulation results for the maze map from the two algorithms color denotes speed of the vehicle where dark blue is zero and red is high speed
jpeg061-Figure7-1.png,difference in velocity between the ml planner and baseline
jpeg061-TableII-1.png,percent reduction compared to baseline
jpeg063-Figure4-1.png,t sne z index of x axis 3 29
jpeg063-Figure5-1.png,diffusion maps z index of x axis 7 08
jpeg063-Figure3-1.png,lle z index of x axis 6 91
jpeg063-Figure2-1.png,pca z index of x axis 7 18
jpeg063-Figure8-1.png,pca test data mapped using own pca parameters divided by x0 dimension median
jpeg063-Figure1-1.png,dna microarray
jpeg063-Figure6-1.png,kaplan meier all training data
jpeg063-Figure7-1.png,pca train data divided by x0 dimension median
jpeg066-Table1-1.png,results of feature tweaking
jpeg066-Figure4-1.png,example of multi lane merge
jpeg066-Figure3-1.png,a well behaved single lane merge
jpeg066-Figure1-1.png,visualization of highway vehicles
jpeg066-Figure2-1.png,linreg baseline losses
jpeg066-Table3-1.png,clustering scores
jpeg067-Figure1-1.png,a measured power draw and histogram of states for dishwasher b state transition model c predicted state sequence via viterbi algorithm
jpeg067-Figure2-1.png,schematic of difference hmm
jpeg067-Figure4-1.png,clustered households via k means clustering k 5 least square error for various k values
jpeg100-Figure1-1.png,data preprocessing
jpeg100-Figure2-1.png,neural network architecture
jpeg100-TableV-1.png,strongest indicators of high skill level
jpeg100-TableVI-1.png,strongest indicators of low skill level
jpeg100-Figure4-1.png,neural network test accuracy for country by actual country
jpeg100-TableVIII-1.png,strongest indicators of an american competitor
jpeg100-TableVII-1.png,strongest indicators of a chinese competitor
jpeg100-TableII-1.png,data set used for rank prediction
jpeg100-TableI-1.png,data format for each submission
jpeg100-TableIII-1.png,data set used for country prediction
jpeg100-TableIV-1.png,accuracy for each model 10 fold cross validation
jpeg100-Figure3-1.png,neural network test accuracy for rank 1 by actual rank
jpeg101-Figure4-1.png,heatmap of perturbation in pixels to input images by fgsm and reluplex on the vanilla network
jpeg101-Figure5-1.png,the frequency of examples that were robust in the fgsm and reluplex sense for ä pixel ranges on the vanilla network
jpeg101-Figure6-1.png,the frequency of examples that were robust in the fgsm and reluplex sense for ä pixel ranges on the adversarially trained network
jpeg101-Figure7-1.png,average minimal ä value for vanilla and robust network in the fgsm and repluplex sense
jpeg101-Figure1-1.png,the frequency of cosine similarity ranges between adversaries generated by fgsm and reluplex on the vanilla network
jpeg101-Figure3-1.png,the frequency of cosine similarity ranges between adversaries generated by fgsm and reluplex on the adversarilly trained network
jpeg101-Figure2-1.png,heatmap of perturbation in pixels to input images by fgsm and reluplex on the vanilla network
jpeg102-Figure1-1.png,real mnist digits sample
jpeg102-Figure2-1.png,synthetic mnist digits sample manually labelled
jpeg102-Figure4-1.png,convolutional nn 4 layer classifier architecture
jpeg102-Figure5-1.png,convolutional nn 5 layer classifier architecture
jpeg102-Figure3-1.png,fully connected nn classifier architecture
jpeg102-Figure6-1.png,particular of the architecture of the modified gans discriminator used as semi supervised classifier showing the two output layers
jpeg102-Table1-1.png,supervised learning classifier results on test accuracy for real and synthetic images
jpeg102-Table2-1.png,supervised learning classifier results on train accuracy on real images
jpeg102-Figure8-1.png,cnn 5l model loss
jpeg102-Figure7-1.png,cnn 5l model accuracy
jpeg102-Table4-1.png,semi supervised modified gans results on train accuracy for label classification of real images
jpeg102-Figure10-1.png,confusion matrix for cnn 5l over synthetic images of test dataset
jpeg102-Table3-1.png,semi supervised modified gans results on test accuracy for label classification of real images
jpeg102-Figure9-1.png,confusion matrix for cnn 5l over real images of test dataset
jpeg102-Figure11-1.png,saliency attention for cnn 5l on a real digit sample
jpeg102-Figure14-1.png,class activation attention class activation attention for cnn 5l on a synthetic digit sample
jpeg102-Figure12-1.png,saliency attention for cnn 5l on a synthetic digit sample
jpeg102-Figure13-1.png,class activation attention class activation attention for cnn 5l on a real digit sample
jpeg104-Table2-1.png,1st hyper parameter search
jpeg104-Table4-1.png,feature influence
jpeg104-Table3-1.png,model performance summary
jpeg104-Table1-1.png,features contributing top variance to each principal component
jpeg104-Figure1-1.png,where the data comes from
jpeg104-Figure2-1.png,2 component pca
jpeg104-Figure4-1.png,left svr rbf results right linearsvr results
jpeg104-Figure3-1.png,neural network performance a test predictions vs test labels b histogram of the error difference between the model predictions and the true test labels
jpeg106-Figure3-1.png,classification algorithm results for voting classifiers
jpeg106-Figure2-1.png,optimized parameters for a rf and b nn models per output task
jpeg106-Figure6-1.png,maps of correct and incorrect classifications for a functionality b water quality and c water quantity
jpeg106-Figure1-1.png,maps of tanzania s hand pump a functionality b water quality and c quantity
jpeg106-Figure4-1.png,micro averaged f1 scores for train and test datasets
jpeg106-Figure5-1.png,mcc for all algorithms
jpeg109-TableII-1.png,table ii
jpeg109-TableI-1.png,table i
jpeg109-TableIV-1.png,table iv
jpeg109-Figure3-1.png,accuracy for basic and parity models with without postprocessing step enforcing demographic parity dp
jpeg109-TableV-1.png,table v
jpeg109-Figure2-1.png,roc curve comparison for basic and opportunity models with without post processing step enforcing equality of opportunity eo
jpeg109-Figure1-1.png,adversarial model setup
jpeg109-TableIII-1.png,table iii
jpeg11-Figure1-1.png,the black dashed line above represents the minimum achievable mse based off inherent noise in metabolic data comparisons of training networks on data from two subjects using a all features b step and emg features c step features only and d emg features only
jpeg111-Figure4-1.png,pca graph for k means
jpeg111-Table1-1.png,summarizes some of the variables explored in respect to their individual correlation to our success metric mn_earn_wne_p10 mean earnings of students working and not enrolled 10 years after entry
jpeg111-Figure2-1.png,accuracy declined at higher levels of earnings
jpeg116-TableI-1.png,results
jpeg116-TableIV-1.png,ablation study
jpeg116-TableII-1.png,confusion matrix
jpeg116-TableIII-1.png,confusion matrix
jpeg117-Figure2-1.png,adaboost algorithm visualized 1
jpeg117-Figure4-1.png,2d visualization of the iclr 2017 dataset where blue dots are accepted papers and red x s are rejected papers
jpeg117-Figure1-1.png,pca visualization of word2vec closer words should appear closer together
jpeg117-Table2-1.png,train and test accuracies for the various models used
jpeg117-Figure3-1.png,fully connected neural network diagram 1
jpeg119-Figure1-1.png,training cross entropy loss by iteration for different nn architectures
jpeg119-Figure2-1.png,final chosen architecture cross entropy loss by iteration
jpeg119-Table2-1.png,5 fold mean accuracy values for expium dataset 5500 tickets on nn architectures
jpeg119-Table6-1.png,linkedin dataset 39817 tickets
jpeg119-Table4-1.png,linkedin dataset 559 tickets out of total 43483 cp arospm
jpeg119-Figure3-1.png,dnn 5 fold cross validation mean train and test accuracy by number of samples
jpeg119-Table3-1.png,expium generated dataset of 5500 tickets
jpeg12-Figure3-1.png,k means based pixel classification
jpeg12-Figure2-1.png,bounding boxes generated using mobilenet ssd single shot multi box detector to track golfer
jpeg12-Figure6-1.png,test classification example the classifier successfully detects the near traversable terrain and target areas of the background and sky are misclassified as well as the cart path
jpeg12-Figure1-1.png,example supervised training example images were hand classified using image editing software and color codes
jpeg12-Figure4-1.png,felzenszwalb segmentation followed with cluster pooling
jpeg12-Figure5-1.png,test classification example the classifier successfully detects the green edges of the background sky and parts of the traversable terrain but mistakenly adds a background layer surrounding the green
jpeg120-TableI-1.png,result for willingness problem
jpeg120-Figure1-1.png,roc curves for willingness problem
jpeg120-Figure3-1.png,roc curves for duration problem xgboost
jpeg120-Figure2-1.png,precision recall curves for willingness problem
jpeg122-Figure3-1.png,lstm configuration
jpeg122-Figure2-1.png,rating distribution of amazon reviews
jpeg122-Table1-1.png,performance of different models
jpeg122-Figure4-1.png,ranking of different models by test accuracy
jpeg122-Figure1-1.png,published papers on sentiment analysis
jpeg125-Figure3-1.png,accuracy of different models and input features on training and test sets
jpeg125-Figure2-1.png,shares per article left and shares per user right histograms generated from the dataset the y axis is on a logarithmic scale
jpeg125-Figure1-1.png,a graph visualization of a small subset of the dataset user nodes left column are shown in the blue while article nodes right column are shown in red for fake articles and green for real articles edges between users indicate follower followee relationships and edges between users and articles indicate articles shared by a user
jpeg127-Figure2-1.png,fights and winplaceperc in different clusters left escape and winplaceperc in different clusters right
jpeg127-Figure1-1.png,damagedealt distribution left walkdistance distribution right
jpeg128-Figure8-1.png,rmse as a function of number of epochs for different kernels
jpeg128-Figure6-1.png,rmse for different number of factors 14 factors seems to be the optimal
jpeg128-Figure7-1.png,rmse as a function of number of epochs for different update schemes
jpeg128-Figure9-1.png,histogram for predictions using matrix factorization
jpeg128-Table1-1.png,top 10 most popular movies by weighted score
jpeg128-Figure1-1.png,rmse for different values of w1 w2 1 w1 histogram for predictions using tf idf similarity
jpeg128-Table2-1.png,movies most similar to skyfall
jpeg128-Figure4-1.png,knn item based ratings prediction histogram
jpeg128-Figure5-1.png,knn user based ratings prediction histogram
jpeg128-Figure3-1.png,knn training rmse with different values of k
jpeg128-Table3-1.png,learning rate with test rmse
jpeg134-Figure3-1.png,diagram of our multi modal model which utilizes the same idea as our wikipedia embedding mlp but also concatenates in the nighlights histogram as input to a more complicated network
jpeg134-Figure2-1.png,diagram of our wikipedia embedding model the ten hyperparameter closest geolocated wikipedia articles are obtained and the doc2vec embeddings are generated and concatenated into one large 3000 dimensional vector we then append 10 nodes to that vector representing the distance between each doc and the point of interest
jpeg134-Figure6-1.png,figure 6
jpeg134-Figure7-1.png,left titles with largest values at indices in embedding most predictive of wealth level 24 182 larger words indicate larger values right admin 2 level prediction vs ground truth for model trained on ghana evaluated on tanzania
jpeg134-Figure1-1.png,left tsne of a subset of ghanaian articles with a cluster of rural village articles noted tsne clusters similar embeddings together middle heatmap of wealth coordinates for ghana with red being wealthier and blue being poorer right distribution of groundtruth wealth data after scaling range between 2 and 2
jpeg134-Figure4-1.png,left comparison of average cross boundary performance for all models right multi modal model results trained on column country tested on row metric pearson s r2
jpeg134-Figure5-1.png,model results trained on malawi and tested on uganda the leftmost graph shows results for a model trained only on doc2vec the centermost shows results for a model trained only on the nighlights imagery and the rightmost shows results for our multi modal model which combines both inputs as is clear the combination of inputs yields the best results
jpeg135-Figure1-1.png,embedding visualizations the top row depicts pca visualizations for embeddings generated on chgminer and the bottom row depicts t sne visualizations for embeddings generated on email eu core both colored by class a represents the canonical graph factorization baseline b represents deepwalk c represents node2vec and d represents our supervised poincar embeddings
jpeg135-Table1-1.png,overall embedding performance this table depicts the performance of varying types of embedding schema on node classification and link prediction with best results typeset in bold link prediction results are reported for train and test sets sizes of training and test sets for both tasks are additionally reported
jpeg137-Table1-1.png,auc for different model and features combination lr logistic regression nb naive bayes rf random forest it seems that the minimal feature has contains most of the predictive power and all models have similar performance
jpeg137-Table4-1.png,auc for tichu prediction with different model feature combinations
jpeg137-Table2-1.png,coefficients for the 4 most important features for grand tichu prediction
jpeg137-Figure3-1.png,with input as 14 identified patterns plus card hand naive bayes performs worse than the other models
jpeg137-Table3-1.png,coefficients for the 6 most important features for tichu prediction
jpeg137-Figure2-1.png,with input as 14 identified patterns plus card hand naive bayes performs worse than the other models
jpeg137-Figure4-1.png,an example of the histogram of tichu success probabilities given a fixed 8 card starting hand each count in the yellow histogram is a random realization of the next 6 cards and the probability is the output of our tichu logistic regressoin model given the 14 card hands since when ptichu 0 5 players won t call tichu the histogram that is smaller than 0 5 is equivalently at 0 5 since ptichu 0 5 has expected return 0 when you call tichu the expected histogram is plotted in blue the equivalent tichu probability p tichu is therefore the average of the blue histogram the red vertical line is the grand tichu probability pgrand the yellow line is the mean tichu probability ptichu and the blue line is the mean equivalent tichu probability p tichu
jpeg137-Figure5-1.png,pgrand p tichu and 0 5 p tichu 0 25 at different grand tichu index ig
jpeg137-Figure1-1.png,an example of a 14 card hand we want to predict the success rate of making tichu with an input hand like this
jpeg138-Table1-1.png,results for models
jpeg138-Figure3-1.png,random forest prediction for lights and washer dryer
jpeg138-Figure1-1.png,true energy disaggregation from redd building 1
jpeg142-Figure2-1.png,resnext model architecture
jpeg142-Figure4-1.png,left incorrect protein docking with rmsd value 16 37 4 0 that was properly predicted as 0 right correct protein docking with rmsd value 0 5 4 0 that was misclassified and predicted as 0
jpeg142-Table5-1.png,resnext50 results on unbalanced dataset lr is the initial learning rate and bs is the mini batch size test results based on trained model with performance seen in
jpeg142-Figure1-1.png,make cubes pipeline a randomly rotate atom positions of docked protein complex b find center of the interaction c find cluster center and all atoms within radius d for each cluster make cubes with 1 cubic angstrom voxels that are 0 if no atom and number for atom type otherwise
jpeg142-Table1-1.png,results for svm regression and corresponding hyperparameter values train scores were computed using 5 fold cross validation c is the regularization parameter and gamma is the inverse of the radius of influence
jpeg142-Table4-1.png,resnext101 results for classification and corresponding hyperparameter values lr is the initial learning rate and bs is the mini batch size
jpeg142-Table2-1.png,results for svm classification and corresponding hyperparameter values c is the regularization parameter and gamma is the inverse of the radius of influence
jpeg142-Figure3-1.png,resnext50 normalized confusion matrix where label is a binary value predicting correctness of protein binding orientation
jpeg142-Table3-1.png,resnext50 results for classification and corresponding hyperparameter values lr is the initial learning rate and bs is the mini batch size
jpeg144-Figure2-1.png,schematic of workflow raw 7 dimensional data for the mixture of t cells and target cells left plotted in twodimensions as fsc a ssc a is projected onto a two dimensional subspace by pca middle and clustered using k means or unsupervised em right each dot represents a single cell training example pca labels are reassigned to original 7 d data far right and compared with the manual gating strategy polygonal boundary drawn by hand on the original dot plot
jpeg144-Figure4-1.png,clustering on the test set result of semi supervised em clustering on two test datasets 10000 examples each using the hyperparameters set by the training set á 0 00004 2 4
jpeg144-Figure3-1.png,a clustering results for k means and unsupervised em with k 4 clusters and varying pca subspace dimension b number of iterations until convergence for both models varying the number of pca principal components or cluster number c 2d pca projection of the labeled dataset using eigenvectors generated from the unlabeled set for semi supervised em d number of iterations until convergence is plotted against varying alpha coefficients for the labeled ground truth examples e clustering results for different alpha values cluster number k 4 top row 2d pca projection bottom row 7 d original data
jpeg144-Figure1-1.png,2d dot plot of flow cytometry data each dot represents a single cell the two axes represent light intensity of a particular color channel log10 scale the overlap in cell populations shown in the middle and right plots makes traditional manual gating using polygonal boundaries impossible despite clear visually distinct populations from victor tieu qi lab stanford university unpublished data
jpeg146-Figure3-1.png,transition matrix for hidden markov model
jpeg146-Table1-1.png,recall and precision results from validation and test sets for selected hyperparameters
jpeg146-Figure5-1.png,example output from supervised learning models for the probability of the initiation class shown in black with read data in blue and orange left logistic regression right neural network
jpeg146-Figure1-1.png,top example data with genes orange and blue are reads black indicates tus 2 bottom example of two read regions before left of arrow and after right of arrow segmentation
jpeg146-Figure2-1.png,transformation of read data for selected gene single location read distribution poisson is shown on the left and the distribution after taking a moving average gaussian is on the right
jpeg146-Figure4-1.png,example output from unsupervised models shown in black with read data in blue and orange dbscan is on the left and values of 0 indicate outliers which are positive results hmm is on the right and the black trace shows the mean of the hidden state at each position
jpeg148-Figure3-1.png,overview of methods used
jpeg148-Figure6-1.png,logistic regression with l2 regularization ë 0 01 run on the dna methylation pca dataset varying the number of the first principal components used overfitting occurs with increasing principal components
jpeg148-Figure7-1.png,peformance of gda on the immune cell fractions dataset with box cox transformations with varied values of ë compared to the baseline gda on this dataset with no transformation the mle estimate of lambda is 9
jpeg148-Figure1-1.png,explained variance vs principal components index plot 70 variance obtained by using 176 principal components out of which 141 had more than 0 1 variance
jpeg148-Figure2-1.png,a the first three principal components with pca applied to the dna methylation dataset b the first three principal components with pca applied after methylmix
jpeg148-Figure4-1.png,best performance of each model on a the dna methylation data with pca b the dna methylation data with methylmix c the immune cell fractions data
jpeg148-Figure5-1.png,confusion matrix of logistic regression on the dna methylation pca dataset using the first 59 principal components
jpeg149-Figure4-1.png,periodical change of learning rate with respect to number of epochs
jpeg149-Figure5-1.png,network structure of combining resnet34 with our own designed features
jpeg149-Figure6-1.png,macro f1 on train and validation datasets the top figure is for the resnet34 model without our own features and the highest validation f1 it reached is 0 4189 at 40th epoch the bottom figure is for the resnet34 model with our own features concatenated and the highest validation f1 it reached is 0 4110 at 49th epoch
jpeg149-Figure1-1.png,the left 4 images are the input of one sample each image has a resolution of 512x512
jpeg149-Figure3-1.png,representative images of how we segment proteins
jpeg149-Figure2-1.png,representative images of how we find relative proportion of protein in nucleus
jpeg149-Table1-1.png,performance of different models evaluated by kaggle leaderboard lb score which is the macro f1 on test set loss is evaluated by binary cross entropy loss
jpeg15-Figure4-1.png,lstm model diagram
jpeg15-Figure3-1.png,simple neural network model
jpeg15-Figure1-1.png,spectrogram before after fir
jpeg15-Figure2-1.png,svr model
jpeg15-Table1-1.png,is adopted from tacotron project 1 after asking 10 stanford students we obtained the mean score of for these three models respectively detailed mean scores rated by 10 students are shown in we put our generated sample wav files in the web https www xiaowang me cs229
jpeg15-Figure5-1.png,seq2seq with attention diagram
jpeg15-Figure6-1.png,training loss after 250 iterations
jpeg153-Figure2-1.png,projection of training and validation data onto first 2 principal components the symmetry of the plots is a likely consequence of data augmentation procedures
jpeg153-Figure3-1.png,roc curves across training and validation datasets for the best performing on the validation dataset model of each model type the roc curve for the overall best performing model is also shown for the test dataset auroc values in the legend correspond to performance on the validation dataset
jpeg153-Table1-1.png,results for all models tested did not converge after 1000 iterations did not converge after 50 iterations
jpeg153-Figure1-1.png,comparison of disorder prevalence between transcription factors and control random sequences from the human genome a motivating example in the study of disordered ppi s is the difference in percent disorder between transcription factors tfs and random sequences in the human genome tfs are thought to recruit transcriptional complexes such as mediator via their disordered domains
jpeg154-Figure6-1.png,comparison of mean accuracies of different input featurizations on t4 lyzozyme mutant classifcation task
jpeg154-Table1-1.png,proteinnet dataset the number of training example proteins is shown for each threshold of sequence similarity to the proteins withheld in the casp11 test dataset the number of examples in the validation and test sets are also shown note that the number of amino acids per protein is on the order of 100s
jpeg154-Figure1-1.png,definition of a target residue s 3d context a to give each target residue a consistent notion of direction we defined orthogonal x y z axes shown as r g b arrows to elicit a change of coordinate basis b example target residue alanine with context of 10 nearest amino acids by ác ác distance sidechains are hidden the á carbons of each residue are shown as small spheres with a rainbow color mapping from n to c terminus note the structure s original basis top left and redefined target centric unit vectors center
jpeg154-Figure4-1.png,the confusion matrix for the model on the training dataset is shown in a for comparison the scores present the blosum62 empirical substitution matrix are shown in b the blosum baseline indicates that some level of confusion is expected based on empirical tolerated substitution frequencies
jpeg154-Figure5-1.png,the first 2 principal components of the embedding vectors for each residue are shown 44 5 and 9 4 variance explained respectively residues are identified by single letter code and colored by chemical property blue polar positive red polar negative green polar neutral black non polar aliphatic purple non polar aromatic yellow cysteine brown other
jpeg154-Figure3-1.png,loss and accuracy metrics over training epochs the average negative log likelihood loss value is shown for the training blue and validation orange datasets are shown in a the accuracy of the model on the validation dataset is shown in b
jpeg154-Figure2-1.png,core cbow architecture employed by the r2v model
jpeg155-Table2-1.png,comparison of performance metrics across all models
jpeg155-Figure6-1.png,bootstrap random forest confusion matrix
jpeg155-Figure1-1.png,this kaplan meier survival analysis curve shows some correlation between complete response and likelihood of survival with p value 0 12
jpeg155-Table1-1.png,re encoding of labels example for site 02
jpeg155-Figure4-1.png,knn confusion matrix k 3
jpeg155-Figure5-1.png,random forest depth vs score
jpeg155-Figure3-1.png,knn k vs accuracy
jpeg155-Figure2-1.png,logistic regression confusion matrix
jpeg159-Figure2-1.png,results
jpeg159-Figure1-1.png,data set summaries
jpeg159-Table2-1.png,slr sli and fpca applied to the data with surgery information
jpeg159-Table1-1.png,sli and fpca applied to the original data
jpeg16-Figure1-1.png,original data and em predictions the accuracy of the predictions is poor
jpeg16-Figure4-1.png,decision boundaries for lr and gda algorithms boundaries from the two algorithms were very similar
jpeg16-Figure5-1.png,results from bagging using random forests
jpeg16-Figure3-1.png,lr confusion matrix on the validation set
jpeg16-Figure6-1.png,analysis results for different maximum depth of dt
jpeg16-Figure2-1.png,analysis results
jpeg16-Figure11-1.png,accuracy on the validation set for specific time periods accuracy improves for individual time periods indicating that hit songs have features unique to their time period
jpeg16-Figure8-1.png,analysis results the lr and nn algorithms were the most successful
jpeg16-Figure10-1.png,features of hit songs released in winter vary from features of other songs
jpeg16-Figure7-1.png,accuracy of nn with increasing epoch the peak accuracy on the validation set with regularization was achieved with approximately 19000 epochs
jpeg16-Figure9-1.png,error analysis for the two strongest performing algorithms
jpeg164-Table1-1.png,summary of properties for the datasets used in this study the final number of genes is the intersection of common genes represented in both datasets
jpeg164-Figure1-1.png,comparison of svm performance depending on parameters scale from violet low score minimum at 0 to bright yellow high score maximum at 1 for example the precision of all the rbf kernel svm
jpeg164-Table2-1.png,accurary and auroc scores for svm classifiers alone versus bayes net predictions for selected go nodes
jpeg164-Figure3-1.png,subtree of go nodes used in bayesian predictions some ids are from out of data go 3
jpeg164-Figure2-1.png,structure of bayes net y i represent svm predictions yi represent the true label of the input example for go node i 3
jpeg166-Figure1-1.png,illustrates a typical setup of a single fiber electromyography emg test electromyography emg measures muscle response or electrical activity in response to a nerve s stimulation of the muscle the test is used to help detect neuromuscular abnormalities an example electrical signal output from the repeated simulation is shown in using many of these muscle simulations it is possible to diagnose myasthenia gravis
jpeg168-TableIV-1.png,ensembled model performance f 1 scoring
jpeg168-Figure5-1.png,receiver operator curve for multi class ensembled model
jpeg168-Figure6-1.png,validation curve of training and cross validation accuracy for varied c 1 ë parameter values in logistic regression
jpeg168-Figure1-1.png,typical mri appearances of improved crmo condition after treatment with pamidronate therapy left mri dated 3 11 14 right mri dates 8 14 14
jpeg168-Figure2-1.png,class distribution of train dev sets before and after augmentation
jpeg168-TableII-1.png,f 1 scores of transfer learning models
jpeg168-Figure4-1.png,confusion matrices for best performing models top multi class softmax visual words ensembled bottom binary logistic visual words ensembled
jpeg168-TableI-1.png,feature set selection over multiple processing strategies
jpeg168-TableIII-1.png,visual bag of words performance f 1 scoring
jpeg175-TableII-1.png,metric graduation rate
jpeg175-TableI-1.png,metric of students progressing to college
jpeg175-Figure1-1.png,principal components analysis using attending college output metric
jpeg175-TableV-1.png,causal inference derivative approximations for a single school
jpeg175-TableIV-1.png,metric average composite sat score
jpeg175-TableIII-1.png,metric average composite 10th grade mcas
jpeg175-Figure2-1.png,causal inference of avg expenditures per pupil vs graduated for a single school athol high school
jpeg18-Figure4-1.png,lstm structure diagram 12
jpeg18-Figure5-1.png,encoder decoder model diagram
jpeg18-Figure11-1.png,encoder decoder model generated music
jpeg18-Figure10-1.png,encoder decoder model average loss
jpeg18-Figure9-1.png,music generated from lstm algorithm
jpeg18-Figure1-1.png,beethoven s fifth symphony music sheet
jpeg18-Figure3-1.png,a midi file played in synthesia
jpeg18-Figure2-1.png,matrix representation of lstm model data
jpeg18-Table1-1.png,results for all the models
jpeg18-Figure8-1.png,lstm model loss
jpeg18-Figure6-1.png,gru model diagram 3
jpeg18-Figure7-1.png,neural network performance on a dataset with ca 10 000 values with 1024 neurons
jpeg180-Figure2-1.png,hybrid siamese network
jpeg180-Figure1-1.png,process for k nn
jpeg180-Figure3-1.png,loss curve
jpeg180-Table1-1.png,results
jpeg183-Table1-1.png,model performance measured by classification accuracy
jpeg183-Figure3-1.png,confusion matrix for logistic regression with tf idf features
jpeg183-Figure2-1.png,typical neural network model architecture
jpeg183-Table2-1.png,model performance of different neural networks measured by classification accuracy number after cnn in the model s name denotes the number of convolutional layers in the model the number following rnn denotes the number of units in lstm layer of the network top1 column denotes the results if considering the top one label predicted by the models top3 if considering top three labels
jpeg183-Figure5-1.png,visualization of word embeddings for different news categories
jpeg183-Figure1-1.png,statistical analysis of dataset a number of samples per category b average number of words in the combined news description
jpeg183-Figure4-1.png,typical model accuracy and loss curves train and dev for neural network models
jpeg184-Figure2-1.png,variational auto encoder
jpeg184-Figure4-1.png,tsne for glove glove makes use of feature space more efficiently grasping more subtle meanings of words
jpeg185-Figure2-1.png,algorithm accuracy results confusion matrix
jpeg185-Figure3-1.png,training curves for classification lstm left and generation lstm right
jpeg185-Figure1-1.png,natural language features
jpeg189-Figure1-1.png,a sample of users ratings histogram of user ratings
jpeg189-Figure5-1.png,predicted ratings using nmf predicted ratings using svd
jpeg189-Figure3-1.png,example of word2vec skip gram model recommendation results similarity scores in parentheses t sne visual representation of a sample of dish names similarity
jpeg19-FigureIV-1.png,3 three different feature vectors of the test set data points music pieces with the vectors reduced to 3d using pca the colors represent true genre labels of each test data point
jpeg19-TableIV-1.png,1 comparison of different model architectures c denotes a convolution layer dc denotes a dilated convolution layer m denotes a max pooling layer a denotes an average pooling layer and f denotes a fully connected layer o denotes the output of the model
jpeg19-TableV-1.png,1 different model architecture s comparison
jpeg19-FigureVI-1.png,1 lr on first layer of dcnn of test data with true left and predicted right genres
jpeg19-FigureIII-1.png,1 2d mfcc array of a classical and a metal music piece from the dataset with the horizontal axis as time and the vertical axis as mfcc values
jpeg19-FigureV-1.png,1 dcnn confusion matrix
jpeg190-Table1-1.png,list of features for entity extraction
jpeg190-Table4-1.png,conditional random field
jpeg190-Table3-1.png,entity recognition metrics for gbt
jpeg190-Table2-1.png,entity recognition metrics for svm rbf kernel
jpeg195-Figure1-1.png,preprocessing flowchart
jpeg195-TableI-1.png,elements
jpeg195-TableII-1.png,predictions of the example text
jpeg195-Figure4-1.png,svm weighted f1 scores and test accuracies across 20 elements
jpeg195-Figure5-1.png,test set accuracy across 20 elements with lstm svm and xgboost
jpeg195-TableIII-1.png,weighted f1 scores of test dataset for 3 topics
jpeg20-Table1-1.png,comparison of classification accuracy between deep softmax autoencoder and baseline two layer neural network
jpeg20-Figure3-1.png,deep softmax autoencoder accuracy curves with the epoch number on the x axis
jpeg20-Figure2-1.png,model architecture of combined deep autoencoder and feed forward multi class classifier referred to as a deep softmax autoencoder
jpeg20-Figure6-1.png,visualization of pca on 4 dim logits as potential encodings
jpeg20-Figure1-1.png,visualization of pca on raw input data
jpeg20-Table2-1.png,objective metrics over a held out test set of 1000 examples for the deep softmax autoencoder
jpeg20-Figure4-1.png,confusion matrix of the deep softmax autoencoder s predictions on a test set of 1000 held out examples
jpeg20-Figure5-1.png,visualization of pca on bottleneck 64 dim encodings
jpeg203-Figure1-1.png,average differences in cosine similarity score over 10 trials between the modified embedding space and the original embedding space blue is all appended features orange is condensed time
jpeg203-Figure2-1.png,comparing our algorithm against native cosal and the labeled data the i j th entry in each grid is the number of tweets that are ranked more similar than tweet j to tweet i
jpeg203-Figure5-1.png,t sne applied to the modified embedding space with the two tweets marked in red
jpeg203-Figure6-1.png,t sne applied to the original embedding space of the 2016 election subset with the three tweets colored
jpeg203-Figure7-1.png,t sne applied to the modified embedding space of the subset with the three tweets colored
jpeg203-Figure4-1.png,t sne applied to the original embedding space with the two tweets marked in red
jpeg203-Figure3-1.png,average user reported similarity scores n 4 measured over randomly selected pairs of tweets from the new embedding space that the model marked as highly similar versus the number of components that are selected blue is all appended features orange is condensed time and green is the original embedding space with pca applied
jpeg205-Figure3-1.png,boxplots of test set errors each model has roughly the same distribution of errors
jpeg205-Figure2-1.png,examples of test set optimized configurations and their corresponding predicted configurations
jpeg205-Figure4-1.png,example of configuration with overlapping transistors that is scaled until constraints are satisfied
jpeg205-Figure5-1.png,predicted configuration of value iteration by discretization method
jpeg205-Figure1-1.png,representation of two particles of possible configurations leading to a more compact design
jpeg206-Figure1-1.png,dataset depiction a discharge vs cycle number for the first 1 000 cycles of our 124 cells the variation in lifetime 150 to 2300 cycles comes from different charging policies red and blue indicate high and low cycle life respectively b a detailed view of a showing only the first 100 cycles a clear ranking of cycle life has not emerged by cycle 100 c histogram of the capacity ratio between cycle 100 and cycle 2 for most cells in this dataset the capacity initially rises in the first 100 cycles suggesting prediction will be difficult
jpeg206-Figure2-1.png,voltage visualizations for feature generation a voltage vs discharge capacity for cycles 1 through 100 for a typical lithium ion battery b discharge capacity as a function of voltage y axis and cycle number xaxis this plot is another way to visualize the data in a c discharge capacity as a function of voltage y axis and cycle number x axis with the second cycle capacity subtracted d baseline subtracted capacity vs cycle number at 2 9 v the peak around cycle 50 can be attributed to a temperature rise in the environmental testing chamber
jpeg206-Figure3-1.png,mean percent error vs number of cycles used in prediction for a elastic net regression b random forest regression and c adaboost regression
jpeg206-Figure4-1.png,mean percent error vs number of cycles used in prediction for a elastic net regression b random forest regression and c adaboost regression
jpeg206-Figure5-1.png,feature weights for elastic net regression as a function of number of cycles used in prediction the weights are normalized by the standard deviation of the features to enable direct comparison
jpeg206-Figure6-1.png,feature importance for random forest regression as a function of number of cycles used in prediction
jpeg206-Figure7-1.png,feature importance for adaboost regression as a function of number of cycles used in prediction
jpeg207-Figure2-1.png,elastic modulus distribution across the dataset
jpeg207-Figure3-1.png,parameter optimization of the random forest regressor
jpeg209-Figure3-1.png,results form the two model problems with variations of the amount of spatial resolution in the data and the amount of noise in the data
jpeg209-Table1-1.png,parameters used for system identification of test systems
jpeg209-Figure2-1.png,solution to test systems and denoising example
jpeg21-Figure3-1.png,confusion matrix for cnn predictions
jpeg21-Figure1-1.png,examples of log scaled mel spectrograms for three different genres
jpeg21-Table1-1.png,accuracy of predictions by model used
jpeg21-Figure2-1.png,cnn architecture
jpeg210-Figure1-1.png,example input variable green chlorophyll vegetation index gcvi at 1 km resolution ranges from 0 to 6 dark to light green shown for july 15 2018
jpeg210-Table4-1.png,additional statistics for the three models c r
jpeg210-Figure2-1.png,map of average fire risk for the period june august 2017 risk was derived using coefficients from logistic regression c r
jpeg210-Table1-1.png,variables used for fire prediction climate variables are italicized reflectance variables are not italicized
jpeg210-Table2-1.png,percent errors by model for training and testing model labels c r and c r indicate which subset of data was used climate reflectance or both the baseline error achieved by classifying all test examples as no fire was 27 19
jpeg210-Table3-1.png,confusion matrices for a logistic regression c r b boosted trees c r and c mlp c r
jpeg211-Figure4-1.png,mean and standard deviation of the errors
jpeg211-Figure5-1.png,error distributions for the baseline and neural network models
jpeg211-Figure3-1.png,training loss at each optimization iteration
jpeg211-Figure1-1.png,quadrotor reference frame
jpeg211-Figure2-1.png,example data motor commands and position trajectories
jpeg211-TableI-1.png,training validation and test dataset losses for each model
jpeg212-Figure3-1.png,procedure for assessing accuracy of vae model by generating random examples random points are sampled from the latent space and decoded the accuracy is defined as the mse between the reconstructed spectrum and the real spectrum of the reconstructed device
jpeg212-Figure2-1.png,left schematic of variational autoencoder showing input and output vectors encoder and decoder portions and latent space right loss function during training
jpeg212-Table2-1.png,correlation between latent variables and decoded layer thicknesses based on a grid sampling of the latent space each entry is a correlation cor the layer thicknesses 2 and 4 correspond to the high index silicon layers which have a larger effect on the spectrum than the three glass layers
jpeg212-Figure6-1.png,application of vae for inverse design a random device is input with the target spectrum example outputs are shown on the right
jpeg212-Figure1-1.png,a schematic of transfer matrix method b representative example of a discretized transmission spectrum
jpeg212-Table1-1.png,vae hyperparameter tuning
jpeg212-Figure4-1.png,example spectra sampled from the test set showing a comparison between the input reconstructed and actual output spectra
jpeg212-Figure5-1.png,left latent space representation of the test set right decoded thickness parameter 2 thickness of the first silicon layer after sampling on a grid in the latent space with the trained vae the latent parameter 2 appears to be strongly correlated with 2
jpeg213-Table1-1.png,sensitivity of test maes on the type of train test split the reaction and composition entries correspond to splits where the model is tested on either reactions or surface compositions respectively that it has not seen during training composition reaction is the method used throughout the main text and is described there random is simply a random split without any constraints
jpeg213-Figure8-1.png,parity plots for rf red gb green and cnn purple
jpeg213-Figure7-1.png,learning curves for rf gb and cnn
jpeg213-Figure9-1.png,feature importance for rf using the full pdos spectra as features black and average feature values for the surface blue and molecular fragment orange
jpeg213-Figure2-1.png,schematic of the data generation process
jpeg213-Figure4-1.png,learning curves for lr rf gb and krr
jpeg213-Figure5-1.png,parity plots for linear regression blue krr orange gb green and rf red
jpeg213-Figure3-1.png,analysis of present calculations in context of previous work a correlation between binding energy of c or o and the 1st moment of the binding surface atom s pdos b correlation between the binding energy of h binding on surface bound o and the 1st moment of the o pdos c no analogous correlation exists for h binding on surface bound c
jpeg213-Figure6-1.png,feature importance of tree based methods
jpeg213-Figure1-1.png,schematic describing the conventional approach to binding energy calculations with dft and the approach explored in the present work
jpeg214-Figure1-1.png,energy landscape of silicon the bandgap is shaded 2
jpeg214-Figure4-1.png,pipeline for model predictions
jpeg215-Figure1-1.png,sem image of biv o42 theoretical progress in photoelectrochemical cell
jpeg215-Figure4-1.png,edge detection methods
jpeg215-Figure3-1.png,data augmentation for high a and low b working distance
jpeg215-Figure5-1.png,svm with hog
jpeg215-Figure6-1.png,shallow cnn architecture
jpeg215-Figure7-1.png,residual learning a building block 3
jpeg215-Table1-1.png,results of different models
jpeg216-Figure3-1.png,a is a core point b and c are border points n is a noise point
jpeg216-Figure2-1.png,three samples of the train set
jpeg216-Figure4-1.png,the left panel shows the image before processing the right panel shows the image after processing
jpeg216-Figure1-1.png,the upper three figures are examples of photons the lower three figures are examples of particles
jpeg217-Figure4-1.png,gam smoothed over dependency plots for solar and wind energy
jpeg217-Figure3-1.png,gam original dependency plots for solar and wind energy
jpeg217-Figure1-1.png,sample weather timeseries data 7
jpeg217-Table1-1.png,rmse errors
jpeg217-Figure2-1.png,neural network architecture
jpeg219-Figure1-1.png,optimization process
jpeg219-Figure3-1.png,cfd simulation of the maewing2
jpeg219-Figure2-1.png,µ1 changes the dihedral angle left and tµ2 µ3u changes the sweep angle right of the maewing2
jpeg219-Figure4-1.png,mesh for the maewing2
jpeg219-Figure5-1.png,on the left global rom approach on the right our proposed piecewise rom approach
jpeg219-Figure6-1.png,offline phase scheme
jpeg219-Figure7-1.png,online phase scheme
jpeg219-Figure11-1.png,error with different numbers of clusters
jpeg219-Figure14-1.png,relative error with different methods
jpeg219-Figure12-1.png,on the left gradient b liftdrag bµ of the training points on the right training points clustered into two clusters
jpeg219-Figure8-1.png,error of different clustering methods
jpeg219-Figure10-1.png,error with different clustering features
jpeg219-Figure13-1.png,testing point labeled after classification
jpeg219-Figure9-1.png,error of different classification methods
jpeg219-Figure16-1.png,on the left silhouette score versus average relative error on the right calinski harabaz index versus average relative error
jpeg219-Figure15-1.png,clustering score correlation with relative error
jpeg22-Table1-1.png,test accuracies trained and tested with audio feature data onehot genres
jpeg22-Table2-1.png,results of tuned rbf kerneled svm on test set a precision b recall c f1 score d support number of songs in playlist
jpeg22-Table3-1.png,train and test accuracy on the toy dataset for various architectures activation functions in parentheses
jpeg22-Figure4-1.png,2d node2vec data on related artists graph d angelo tim mcgraw and bobmarley and thewailers are seed nodes plotting 4 nearest neighbors of each isolated clusters demonstrate the lack of similarity in these groups
jpeg22-Figure1-1.png,ratio of audio features to genre features for a sample track x i
jpeg22-Table4-1.png,train and test accuracy on various users using final neural network architecture jacob s dataset is relatively separable while myles playlists are heavily overlapping in sound
jpeg22-Figure3-1.png,user playlists mapped to two dimensions via pca we can see that jacob s set is far more separable than myles even in two dimensions
jpeg22-Figure2-1.png,results of training neural network with one hidden layer sigmoid activation and logsoftmax output layer minimizing nll loss with l2 regularization over 150 epochs
jpeg222-TableIII-1.png,evaluation results for dcgan models
jpeg222-Figure2-1.png,synthetic 1002 pixel image after processing white pore black solid
jpeg222-TableII-1.png,data parameters
jpeg222-Figure1-1.png,high level architecture of dcgan
jpeg222-Figure4-1.png,effect of changing a generator activation function from relu to leakyrelu b loss function from log loss to wasserstein distance
jpeg222-Figure5-1.png,discriminator loss while training dcgan 2
jpeg222-Figure3-1.png,discriminator and generator loss function over 60 epochs for 36 864 training images numbers correspond to sequence images shown to the right
jpeg223-Figure1-1.png,autonomous spacecraft motion planning
jpeg223-Figure3-1.png,hyperparameters tuning asc nn top left esc nn top right isc nn bottom
jpeg223-Table1-1.png,accuracy results
jpeg223-Figure2-1.png,pareto front
jpeg227-Figure4-1.png,completeness score comparison clusters obtained from the training and validation sets for different algorithms and appliance types
jpeg227-Figure5-1.png,histogram of the entropy of load profiles for different consumers for each appliance type
jpeg227-TableIII-1.png,mean squared errors mse for predicting consumer responsiveness to price
jpeg227-TableII-1.png,features used for predicting consumer responsiveness to price
jpeg227-TableI-1.png,dataset size for each appliance and the type of analysis applied to each v variability a availability f flexibility
jpeg227-Figure1-1.png,cluster assignments for start time distributions of clothes washers for different consumers obtained with k means clustering k 5
jpeg227-Figure3-1.png,intra cluster variation based on the kl divergence for each unsupervised learning algorithm as a function of the number of clusters
jpeg227-Figure2-1.png,increase in load availability from consumer segmentation for different appliances and numbers of clusters
jpeg229-Figure3-1.png,correlation matrix
jpeg229-Figure5-1.png,final results
jpeg229-Figure4-1.png,fcnn structure
jpeg233-Figure3-1.png,permittivities and predicted fields for samples in the test set with good typical and poor performance the lower right panel shows the predicted fields for a uniform random permittivity input from a distribution the model was not trained on
jpeg233-Figure2-1.png,evolution of the predicted field as the network is trained on a single permittivity input the top panel of each image depicts the permittivity at each point grey the true electric field from an fdfd simulation blue and the predicted field at the given iteration orange the bottom panel depicts lm at each iteration an animated version of this figure which is more informative and which we encourage readers to view is available online at https gfycat com testywanisopod
jpeg233-Figure1-1.png,final architecture for the neural maxwell solver the model takes as inputs a vector of permittivities three successive convolutional layers kernel sizes 5 7 9 channels 32 64 128 output into two appropriately sized dense layers this outputs into three successive transposedconvolutional layers kernel sizes 9 7 5 channels 128 64 32 expanding the signal to the original dimensions of dropout layers with p 0 1 and relu activations follow all but the last layer
jpeg241-Figure2-1.png,diagram of the ensemble voting model and convolutional neural network
jpeg241-Figure3-1.png,the head movement data of two high anxiety participants one participant our best model classified correctly as having high anxiety the other was misclassified
jpeg241-Figure1-1.png,plot of head movement data across pitch yaw and roll for two tracking channels for each of two participants one participant red was recorded to have low anxiety at the time of the survey while the other participant purple was recorded to have high anxiety
jpeg241-Table1-1.png,high anxiety patient classification results on 30 experiences from the held out patient test set best model in each category baselines simple classifiers ensembles cnn is bolded
jpeg243-Figure4-1.png,value gradient architecture using a single hidden layer
jpeg243-Figure5-1.png,evolutionary strategies
jpeg243-Figure8-1.png,vanilla e a2c
jpeg243-Figure6-1.png,weighted combination of candidates
jpeg243-Figure7-1.png,maximum candidate parameters
jpeg243-Figure11-1.png,a2c
jpeg243-Figure14-1.png,evolutionary a2c
jpeg243-Figure12-1.png,es
jpeg243-Figure13-1.png,vanilla e a2c
jpeg243-Figure2-1.png,actor critic architecture sutton and barto 1998
jpeg243-Figure10-1.png,mountain car problem
jpeg243-Figure9-1.png,evolutionary a2c
jpeg248-FigureV-1.png,final portfolio value under different soft buy sell value
jpeg248-FigureIV-1.png,plots of reward and cost against training steps the first row are the cost and reward of aapl and second row are the cost and reward of goog the plummets and spikes are due to the random explorations
jpeg248-FigureVI-1.png,final portfolio value of facebook twitter and nvidia
jpeg248-FigureIII-1.png,graph of our model stockagentdeepqnetwork
jpeg248-FigureII-1.png,processed stock data for apple
jpeg248-FigureVIII-1.png,plots of reward against training steps using sadqnbold with í 1 ã 0 8 and ehc 0 25
jpeg248-FigureIX-1.png,plots of reward and cost against training steps the stock is based on aapl
jpeg248-FigureVII-1.png,final portfolio value under different exploration rates
jpeg248-FigureI-1.png,lunar lander environment
jpeg249-Table2-1.png,evaluation results over 100 episodes training time and model size information in a four ball environment
jpeg249-Table1-1.png,evaluation results over 100 episodes training time and model size information in a two ball environment
jpeg249-Figure2-1.png,average rewards over 1000 training episodes for a3c with continuous action and discrete action in a four ball environment
jpeg249-Figure1-1.png,average rewards over 1000 training episodes for qtable dqn a3c with continuous action a3c with discrete action and random policy in a two ball environment
jpeg250-Figure5-1.png,two different attack mechanisms were used random perturbations and adding noise to facial landmarks
jpeg250-Figure6-1.png,an training image from the lfw datasert with noise clustered near facial landmarks
jpeg250-Figure7-1.png,facial landmark recognition achieves limited success on hamid karzai potentially due to his beard
jpeg250-Figure1-1.png,on the left the adversarial patch is placed next to a banana causing classifier to predict with high confidence that the image contains a toaster 2
jpeg250-Figure4-1.png,non adversarial training flow
jpeg250-Figure3-1.png,training sample from the kaggle facial keypoints dataset 7
jpeg250-Figure2-1.png,example photo from the lfw dataset
jpeg250-TableII-1.png,model performance on raw images noisy images images and images with obscured landmarks this model was trained on classes limited to 20 images each
jpeg250-TableI-1.png,model performance on raw images noisy images images and images with obscured landmarks this model was trained on all training images and thus had imbalanced class sizes such as george w bush which contained 500 images and others that had only 20
jpeg250-TableIV-1.png,average confidence of predictions on raw images noisy images images and images with obscured landmarks this model was trained under the same conditions as reported in
jpeg250-TableIII-1.png,adversarial training model performance on raw images noisy images and obscured landmarks this model was trained on both raw and perturbed inputs
jpeg251-Table1-1.png,final results from cp averaged across 5 trials
jpeg251-Figure3-1.png,initial state of bw
jpeg251-Figure2-1.png,initial state of cp
jpeg251-Figure5-1.png,episodic sum of rewards in 10 s of iterations over training across 5 trials each
jpeg251-Figure1-1.png,diagram of es ppo and max ppo
jpeg251-Figure4-1.png,episodic sum of rewards over training cp in 10 s of iterations across 5 trials each vertical bars are the standard deviation
jpeg254-Figure3-1.png,estimated frames over time from generator notice how it can account for the passage of both time and space
jpeg254-Figure2-1.png,frames generated by the vae on bottom corresponding to input frames shown on the top
jpeg254-Figure5-1.png,non adversarial and adversarial rewards in a sample epoch
jpeg254-Figure1-1.png,amore architecture for single frame
jpeg254-Figure4-1.png,amore architecture across experiments
jpeg255-Figure1-1.png,example of k means clustering of location id 261 the left image shows the pickup locations latitude and longitude that fall within location id 261 the right image shows the result of k means clustering of these trips into 10 clusters
jpeg255-Figure2-1.png,architecture of our fully connected network
jpeg255-TableI-1.png,bucketed labels
jpeg255-TableII-1.png,prediction accuracy for each model
jpeg255-Figure3-1.png,heatmap representation of ground truth activity left versus predicted activity right the lighter the color the heavier the activity despite not being able to predict the exact magnitude of activity well our model is able to capture the relative activity between different locations
jpeg256-Figure5-1.png,dqn algorithm generates pacing rate and remaining budget
jpeg256-Figure6-1.png,ddpg algorithm generates pacing rate and remaining budget
jpeg256-Figure4-1.png,dqn algorithm generates pacing rate and remaining budget
jpeg256-Figure3-1.png,smoothed baseline algorithm generates pacing rate and remaining budget
jpeg256-Figure2-1.png,baseline algorithm generates pacing rate and remaining budget
jpeg256-Figure1-1.png,real time bidding in advertising
jpeg258-Table2-1.png,anomalous labels
jpeg258-Figure3-1.png,full method workflow diagram
jpeg258-Figure2-1.png,a k means clustering 6 clusters b dbscan
jpeg258-Figure6-1.png,annotated results of the embed and cluster process applied to the clusters in cluster subplot relationships are orange figure 6 a red figure 6 b purple figure 6 c green figure 6 d
jpeg258-Table1-1.png,dataset statistics
jpeg258-Figure1-1.png,k means onset to convergence 12
jpeg258-Figure4-1.png,a original clustering 6 clusters b anomalies removed from graph and re embedded before another clustering
jpeg258-Figure5-1.png,embedding and clustering of top sales ranked books
jpeg258-Table3-1.png,examples of clusters identified
jpeg261-TableI-1.png,paysim dataset statistics
jpeg261-Figure1-1.png,pca decomposition of paysim data
jpeg261-TableII-1.png,dataset split details
jpeg261-Figure3-1.png,transfer precision recall f1 trend for increasing fraud class weights
jpeg261-TableIII-1.png,dataset split details
jpeg261-Figure2-1.png,cash out precision recall f1 trend for increasing fraud class weights
jpeg261-TableV-1.png,class weights
jpeg261-TableVI-1.png,results on train set
jpeg261-Figure4-1.png,cv set precision recall curve
jpeg261-Figure5-1.png,train set precision recall curve
jpeg261-TableIV-1.png,results on cv set
jpeg261-TableVIII-1.png,confusion matrices
jpeg261-Figure6-1.png,test set precision recall curve
jpeg261-TableVII-1.png,results on test set
jpeg281-Table1-1.png,table 1
jpeg281-Table2-1.png,table 2
jpeg281-Figure4-1.png,spectral co clustering
jpeg281-Figure3-1.png,nmtf nonnegative matrix trifactorization
jpeg281-Figure2-1.png,k means
jpeg281-Figure1-1.png,figure 1
jpeg289-Figure4-1.png,figure 4
jpeg289-Figure3-1.png,demonstrates the use of a particular ingredient soy sauce across various cuisines this example shows a strong connection between a small set of countries and an ingredient
jpeg289-Table1-1.png,table 1
jpeg296-Figure1-1.png,uplift modelling creation process
jpeg296-Figure2-1.png,features in hillstorm dataset
jpeg296-Figure4-1.png,qini curve rad07
jpeg296-Figure3-1.png,uplift modelling based on 2 different models
jpeg296-Table1-1.png,model accuracy on test set without using one hot vector representation for data representation
jpeg296-Table2-1.png,model accuracy on test set with one hot vector representation for data representation
jpeg296-Figure5-1.png,loss function during model training for the case when we we used one hot vector representation for training data and a 3 layer nn for training
jpeg296-Table3-1.png,f scores for the three models
jpeg296-Figure6-1.png,roc curves for predictive response modelling
jpeg296-Figure7-1.png,qini curve for uplift modelling for train and test data
jpeg3-Figure1-1.png,overview of the features used for a game between the golden state warriors gsw and the oklahoma city thunder okc
jpeg3-Table1-1.png,model results
jpeg3-Figure4-1.png,collaborative filtering hyperparameter
jpeg3-Figure5-1.png,neural network training curve
jpeg3-Figure6-1.png,lstm training curve
jpeg3-Figure2-1.png,overview of neural network architecture overview of lstm architecture
jpeg31-Table1-1.png,key results from experiments models are distinguished by input type whereby each training examples were either defined as individual frames or by video i e sequence of frames the error metric reported is rmse of gdi score
jpeg31-Figure2-1.png,flow of information for gdi prediction video is captured by gillette children s hospital and processed by densepose 15 these densepose frames are then passed through gdi net to predict gdi score
jpeg31-Figure1-1.png,a sample rgb image left processed by densepose right each pixel is assigned to a corresponding point on a human skin mesh 6
jpeg31-Figure3-1.png,top learning curve for top performing model middle regression plot comparing true gdi vs predicted gdi on the validation set bottom architecture of top performing model each frame is separately processed by the 2d cnn and concatenated in sequence before passing to the lstm
jpeg33-Table1-1.png,logistic regression results
jpeg33-Figure4-1.png,a sketch of how we carried out transfer learning
jpeg33-Figure5-1.png,linear regression s confusion matrix
jpeg33-Figure6-1.png,banana doodles hockey stick doodles
jpeg33-Table6-1.png,transfer learning training time results
jpeg33-Table5-1.png,transfer learning accuracy results
jpeg33-Figure1-1.png,examples of the data
jpeg33-Figure3-1.png,cnn versions a sketch of how we progressively simplified our cnn for doodle classification
jpeg33-Figure2-1.png,cnn architecture
jpeg33-Table4-1.png,cnn results with binarized input features
jpeg33-Figure8-1.png,confusion matrix for v2
jpeg33-Table3-1.png,cnn results with non binarized input features
jpeg33-Figure9-1.png,cnn training plot
jpeg34-Figure1-1.png,a conv lstm b bi convlstm cell
jpeg34-Figure4-1.png,sequence 1 silog rmse a1 a2 a3 from top to bottom
jpeg34-Figure5-1.png,sequence 6 silog rmse from top to bottom
jpeg34-Figure3-1.png,x lhs y y rhs
jpeg34-Figure2-1.png,unoit detail
jpeg35-Figure4-1.png,an example of lstm visualized side by side with the relevant equations 14
jpeg35-Figure5-1.png,cnn rnn densenet based model
jpeg35-Figure8-1.png,performance plotted against various models
jpeg35-Figure7-1.png,logistic regression lr rnn r cnn c nn n svm l inear p oly 2 s igmoid rb f yellow best model blue best model age
jpeg35-Figure1-1.png,example time series plots from two patients with ica parcellation a female non heavy drinker b male heavy drinker
jpeg35-Figure3-1.png,craddock parcellation examples 11
jpeg35-Figure2-1.png,bold signal histograms a normal b skewed
jpeg35-Figure6-1.png,confusion matrix for a representative result of a single k fold cross validation instance for logistic regression ica
jpeg370-Table1-1.png,all periods classification metrics
jpeg39-Figure2-1.png,inceptionv3 network
jpeg39-Figure5-1.png,misclassified images
jpeg39-Figure6-1.png,image augmentation
jpeg39-Figure7-1.png,inceptionv3 minus top layer no data augmentation left data augmentation with shift flip middle data augmentation with flip and zoom right
jpeg39-Figure1-1.png,example structural images
jpeg39-Table1-1.png,summary of accuracies for classification models
jpeg39-Figure4-1.png,learning curve for retrained inceptionv3 left and example tensorboard plot right
jpeg39-Figure3-1.png,tuning of the parameter for inceptionv3 svm
jpeg400-Figure2-1.png,performance of predictive models on yeast mpra dataset a true x axis and predicted y axis expression levels for constructs with different numbers of gnc4 motifs b deep learning models reproduce experimentally the observed behaviors of constructs with different numbers of gnc4 motifs in response to increasing aa c d deeplift importance score for a construct containing three weak gcn4 tfbss c and another one containing three strong gcn4 tfbss correctly identify motifs driving regulatory activity
jpeg400-Table4-1.png,evaluation of sequences produced from 5000 epochs of training for ã 0 5 on the shrapr mpra dataset
jpeg400-Table3-1.png,evaluation of sequences produced from 5000 epochs of training for various ã on the yeast mpra dataset
jpeg400-Table1-1.png,architectural search of deep learning cnn models for predicting regulatory activity from sequence each architectural property and the corresponding distribution it was drawn from in the random search are shown note that for properties relevant to multiple layers such as units per dense layer n independent samples were drawn from the distribution corresponding to the relevant number of layers
jpeg400-Figure1-1.png,outline of deep learning strategies for predicting activity and de novo generating functionally active dna sequences a the regression training phase of the sequence generation pipeline takes a set of sequences and trains a number of models to predict expression driven by these sequences b structure of the gan component of the sequence generation pipeline which creates de novo predicted to be functionally active sequences
jpeg400-Table2-1.png,top models from random architecture search for the yeast mpra and sharpr mpra datasets for feature importance scoring we sum the deeplift importance scores ascribed to tfbss and divide by the total importance of the input sequence we sum this score over all sequences in both the training and test datasets
jpeg41-Figure3-1.png,architecture for our cnn
jpeg41-Figure8-1.png,plot of inference time vs test error
jpeg41-Figure7-1.png,sample paintings for claude monet far left childe hassam left giovanni battista piranesi right and gustave dore far right
jpeg41-Figure1-1.png,example input paintings by monet matisse aivazovsky and sargent
jpeg41-Figure2-1.png,examples of feature extraction
jpeg41-Figure4-1.png,accuracy across models a svm b cnn
jpeg41-Figure6-1.png,confusion matrices x axis predicted class y axis true class
jpeg42-TableV-1.png,structure of residual cnn
jpeg42-TableVI-1.png,structure of cyclegan with simple cnn layers
jpeg42-Figure8-1.png,model losses of cyclegan
jpeg42-Figure7-1.png,generated test images from cyclegan
jpeg42-TableVII-1.png,inception score of various models
jpeg42-TableII-1.png,useful tags for celeba dataset
jpeg42-Figure1-1.png,sample images from the two datasets
jpeg42-TableI-1.png,tags for hk polytechnic dataset
jpeg42-Figure2-1.png,vanila gan model for beard removal
jpeg42-Figure4-1.png,generated images from multilayer perceptron
jpeg42-Figure5-1.png,generated images from simple cnn
jpeg42-Figure3-1.png,cycle gan model for beard removal
jpeg42-Figure6-1.png,generated images from residual cnn
jpeg44-Figure2-1.png,a the original uncompressed image 26 966 bytes which is a gan generator output b jpeg compression left reduced to 10 quality 1671 bytes right reduced to 1 quality 661 bytes c our method standard quantization 478 bytes extreme quantization 240 bytes
jpeg44-Table1-1.png,summary of the metrics obtained from the baselines and our approach gan reversal on our test set
jpeg44-Figure1-1.png,a simplified graphical explanation of the gan reversal process
jpeg44-Figure3-1.png,a an uncompressed unseen image from our test set 29 171 bytes b k means k 4 compressed image c jpeg compression left reduced to 10 quality 1 030 bytes right reduced to 1 quality 457 bytes c our gan reversal scheme extreme quantization 450 bytes
jpeg51-Figure1-1.png,behavioral cloning trajectory drifting image from 6
jpeg51-Figure2-1.png,donkey car
jpeg51-Figure3-1.png,cnn autopilot
jpeg51-Figure4-1.png,manual action correction by expert
jpeg51-Figure5-1.png,experiment results
jpeg51-Figure6-1.png,ð1
jpeg51-Figure8-1.png,ð3
jpeg51-Figure7-1.png,ð2
jpeg51-Figure9-1.png,ð1 merged
jpeg52-Figure3-1.png,unit translates an image from the gta5 style domain to the cityscapes style domain
jpeg52-Figure2-1.png,ground truth examples from the cityscapes and gta5 datasets
jpeg52-Table2-1.png,miou scores for our two models evaluated on the cityscapes dataset after 100 epochs of training from scratch
jpeg52-Figure1-1.png,selected images from cityscapes and gta5 datasets
jpeg52-Table1-1.png,miou scores for our two models evaluated on the cityscapes dataset after 120 epochs of training using transfer learning
jpeg52-Figure4-1.png,a comparison of our data pipelines for our baseline
jpeg52-Figure5-1.png,baseline and unit mapped show similar training loss curves and miou curves for 120 epochs
jpeg53-Figure2-1.png,spectrogram orange dots stand for the peaks of power over time horizontal axis and frequency vertical axis the brighter dots are the more powerful
jpeg53-Figure1-1.png,convolutional recurrent neural network c rnn
jpeg53-Figure3-1.png,confusion matrix on test set horizontal axis represents predicted labels of songnet vertical axis represents the groundtruth diagonal numbers indicate correctly classified samples
jpeg56-Figure3-1.png,a classification model b r cnn localization baseline c cam localization model
jpeg56-Figure8-1.png,iou distribution in 100s
jpeg56-Figure6-1.png,train and validation accuracy
jpeg56-Figure7-1.png,examples of false negative predictions
jpeg56-Figure9-1.png,examples of prediction with no overlapping
jpeg56-Figure1-1.png,left lung with pneumonia mid unhealthy diseased lung no pneumonia right healthy lung
jpeg56-Figure2-1.png,left original input image mid predicted lung location right segmented lung
jpeg56-Table1-1.png,classification accuracy for our model baselines
jpeg56-Figure4-1.png,left original image mid segmented lung right class activation heatmap when we feed in only original image
jpeg56-Figure5-1.png,heatmap with threshold cutoff value of left 0 2 mid 0 45 right 0 7
jpeg58-Figure1-1.png,example images from dataset kaggle 2018
jpeg58-Figure2-1.png,feature engineering examples
jpeg58-Figure3-1.png,transferred learning cnn tl cnn model framework
jpeg58-Table1-1.png,test accuracy of machine learning algorithms with different feature extraction techniques his stands for color histogram ha means haralick textures and hu ia abbreviation of hu moments
jpeg58-Figure4-1.png,simple cnn sim cnn model framework
jpeg58-Figure5-1.png,box plot for test accuracy of machine learning algorithms
jpeg58-Figure6-1.png,training loss and accuracy of cnn models
jpeg58-Figure8-1.png,normalized confusion matrix
jpeg58-Figure7-1.png,threshold scanning of tl cnn and sim cnn
jpeg59-Figure2-1.png,recurrent neural network to process captions
jpeg59-Figure4-1.png,query dog
jpeg59-Figure5-1.png,query a guy riding a bike next to a train
jpeg59-Figure1-1.png,19 layer vgg network without last fully connected layer
jpeg59-Table1-1.png,recall k in results on the test set for glove we use it as initialization of the word vectors and fine tune it in the training
jpeg59-Figure3-1.png,r 10 measure variation throughout the epochs left and its dependency on the length of the caption right
jpeg6-Figure1-1.png,we achieved our best results with this architecture our architecture had size layer1 n 512 layer2 512 k where n is the number of data features ranging from 12 52 and k 18 is the number of class labels we use relu activation softmax final activation cross entropy loss and applied dropout to avoid overfitting
jpeg6-Figure2-1.png,training and test results for all methods for both full and limited feature sets
jpeg6-Figure5-1.png,confidence matrix for boosted decision trees on limited featureset the four most frequent misclassifications are highlighted in yellow these represented misclassifications between ascending and descending stairs and between vacuum cleaning and ascending descending stairs these misclassifications are expected because of the relative similarity in hand motions and heart rate between these activities
jpeg6-Figure4-1.png,left training and validation curves for limited feature set using ordinary decision trees right training loss and accuracy for the multilayer perceptron neural net when trained on the reduced feature set
jpeg6-Figure3-1.png,validation curves for a given range of regularization constants c for the svm and logistic regression models training curves for logistic regression cannot be seen easily as they lie just above the validation curves
jpeg61-Figure1-1.png,grayscale video of the interaction between a male and female fly that served as the input for this project
jpeg61-Figure2-1.png,the dataset consisted of individual video frames labeled with various key points on both flies
jpeg61-Figure4-1.png,the flycount pipeline stage
jpeg61-Figure5-1.png,the vs pipeline stage
jpeg61-Figure3-1.png,the image processing pipeline developed in this project
jpeg61-Table1-1.png,model accuracy summary
jpeg61-Table2-1.png,timing breakdown for the image processing pipeline
jpeg61-Figure8-1.png,the first two principal components of the hog descriptor used in the orientation pipeline stage for female flies
jpeg61-Figure10-1.png,male fly wing angle as determined by the wingangle stage of the processing pipeline
jpeg61-Figure6-1.png,the orientation pipeline stage
jpeg61-Figure7-1.png,the wingangle pipeline stage
jpeg61-Figure9-1.png,the wing angle can be predicted fairly well with even just the first principal component of the hog descriptor
jpeg65-Figure1-1.png,sample image
jpeg65-Figure3-1.png,hog features
jpeg65-Figure2-1.png,blob features
jpeg65-TableII-1.png,accuracy of feature extraction methods for support vector machine classifiers
jpeg65-Figure4-1.png,learning rate finder loss
jpeg65-Figure5-1.png,sgdr cosine annealing
jpeg65-TableI-1.png,multi class classifier accuracy on raw data
jpeg65-Figure8-1.png,resnet18 train val accuracy
jpeg65-Figure10-1.png,resnet34 train val accuracy
jpeg65-Figure6-1.png,svm svc confusion matrix
jpeg65-Figure7-1.png,resnet18 training loss
jpeg65-Figure9-1.png,resnet34 training loss
jpeg65-Figure11-1.png,resnet34 adam confusion matrix
jpeg65-TableIII-1.png,accuracy of cnn models
jpeg67-Table2-1.png,comparison of mean validation set rmse for different loss functions and transformations
jpeg67-Figure4-1.png,prediction vs ground truths for cs107 autumn 2018
jpeg67-Figure1-1.png,full dataset white train yellow test seen courses green test unseen courses
jpeg67-Figure3-1.png,sqhuber loss
jpeg69-Figure2-1.png,annualized return vs percentage of loans invested on training top and test bottom sets
jpeg69-Figure1-1.png,feature correlation heatmap
jpeg7-Figure2-1.png,in a recursive architecture inputs to later hidden layers are concatenated with outputs of earlier hidden layers the idea
jpeg71-Figure2-1.png,from left to right these panels display high spatial autocorrelation clustering minimal spatial autocorrelation randomness that tends to indicate market disequilibrium and spatial anticorrelation
jpeg71-Figure5-1.png,engineered features of spatial clustering were xgboost s most important predictors lending credibility to the theory of endogenous gentrification
jpeg71-Table3-1.png,the confusion matrix for the random forest on the income distribution response reveals that the classifier predicted positive 86 of the time
jpeg71-Table1-1.png,the census discretizes income reporting into bins that are more granular towards the lower end of the income scale
jpeg71-Figure1-1.png,in this census tract the income distribution skews towards affluence and becomes less trimodal between year 1 and year 2 this shift indicates that gentrification occurred
jpeg71-Table2-1.png,the grid search values of ë for xgboost and c for 1 penalized logistic regression reveal that regularization greatly impacted model performance in classifying tracts according to the change in the monthly cost of housing this is likely due to the large feature dimensionality
jpeg71-Figure4-1.png,no model outperformed the no information classifier on the income distribution shift response
jpeg71-Figure3-1.png,xgboost and the ensemble model performed best on the change in monthly housing cost response with a 10 accuracy improvement over the no information classifier
jpeg72-Figure2-1.png,the general model structure
jpeg72-Table2-1.png,precision recall of svm on test set
jpeg72-Figure3-1.png,prediction of facebook stock movement in 2017
jpeg72-Figure1-1.png,label vs two principal components of news
jpeg72-Table1-1.png,experiments and results
jpeg76-TableI-1.png,market features weekly dataset
jpeg76-TableII-1.png,fundamental features monthly dataset
jpeg76-Figure1-1.png,nn architecture second hidden layer only for the market variables model
jpeg76-TableIV-1.png,binary classification accuracy
jpeg76-Figure3-1.png,conditional density of 3 month mexican t bills
jpeg76-TableIII-1.png,selected parameters
jpeg76-Figure2-1.png,confusion matrix of the svm model on the market variables dataset
jpeg76-TableV-1.png,continuous target accuracy
jpeg76-Figure4-1.png,confusion matrix of the ridge model on the market variables data
jpeg76-Figure7-1.png,variable importance for ridge regression on the market variables dataset under the continuous target framework
jpeg82-Figure2-1.png,architecture of the fcnn
jpeg82-Table4-1.png,confusion matrix for lgbm
jpeg82-Table3-1.png,confusion matrix for fcnn
jpeg82-Figure1-1.png,closing prices by quantiles
jpeg82-Table1-1.png,validation accuracy score and test score of the 3 models
jpeg82-Figure5-1.png,roc curve for the lgbm
jpeg82-Figure3-1.png,roc curve for logistic regression roc curve for the fc neural network
jpeg86-Figure1-1.png,condos sale price distribution on the map
jpeg86-TableI-1.png,accuracy and variance metrics for different methods
jpeg86-Figure2-1.png,r2 as a function of training set size for lr and gb
jpeg88-Figure3-1.png,distribution of funding dates
jpeg88-Figure2-1.png,imbalanced dataset 1 ipo or acquired 0 closed or operating
jpeg88-Table4-1.png,metrics results
jpeg88-Figure4-1.png,roc curve
jpeg88-Figure5-1.png,confusion matrix of random forest confusion matrix of knn
jpeg88-Figure1-1.png,selected features and corresponding examples
jpeg88-Table1-1.png,dataset split and up sample
jpeg88-Table2-1.png,setting of hyperparameters tuning
jpeg88-Table3-1.png,summary of hyperparameters
jpeg89-Figure5-1.png,outcome vs prediction in training vs test sets
jpeg89-Figure6-1.png,variable importance in random forests on aud complete sample contiguous split
jpeg89-Figure2-1.png,list of features in each of aud jpy usd
jpeg89-Figure4-1.png,summary of mses from models on contiguous split
jpeg89-Figure1-1.png,covered three month interest rate parity condition
jpeg96-Figure2-1.png,comparative histograms of predicted and actual prices for the top 3 models svr kmc and nn
jpeg96-Figure1-1.png,geographic spread of price labels with filtered outliers
jpeg98-Figure3-1.png,cnn architecture
jpeg98-Figure11-1.png,saliency maps for apple blueberry and onion examples
jpeg98-Figure8-1.png,cnn loss cnn map 3
jpeg98-Figure10-1.png,cnn map 3 score distribution by category
jpeg98-Figure1-1.png,sample doodles of a sock elbow and carrot left to right from the training dataset
jpeg98-Figure2-1.png,two of the centroids created by running k means clustering using k means initialization on the bear training examples
jpeg98-Table1-1.png,map 1 and map 3 scores for each method across train dev test
jpeg98-Figure4-1.png,centroids for the circle and octagon categories
jpeg98-Figure5-1.png,centroids for apple blueberry and onion left to right
jpeg98-Figure6-1.png,knn rank weighting k 29 map 3 score distribution by category
jpeg98-Figure7-1.png,map 3 scores plotted against different values of k for knn with weighted voting rank distance
